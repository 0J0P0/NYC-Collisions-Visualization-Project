{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Authors**:\n",
    "\n",
    "- Juan Pablo Zaldivar\n",
    "- Enric Millán\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second project, the focus is on analyzing collision data in New York City during the summer of 2018. The primary objective is to develop a comprehensive interactive visualization that can address several key questions regarding the nature and patterns of these collisions. With the use of datasets related to collisions, weather conditions, and the New York City map, we aim to explore various facets, including ...\n",
    "\n",
    "This file contains all the steps required to ensure reproducibility of steps leading from raw data to a clean dataset. The project is divided in three parts: the first part corresponds to the preprocessing of the data, the second part corresponds to the visualization desing process and the third part corresponds to the implementation of the visualization in the streamlit app to answer the questions.\n",
    "\n",
    "The datasets are as follows:\n",
    "\n",
    "- Collision Dataset: Extracting and filtering collision data specifically from June to September of 2018. This involves selecting relevant columns, handling missing or inconsistent data, and ensuring data quality.\n",
    "\n",
    "- Weather Dataset: Locating and incorporating weather data corresponding to the time frames and areas of interest.\n",
    "\n",
    "- New York City Map: Acquiring a suitable map of New York City to overlay geographical information related to collision locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset obtention and description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Collisions dataset (`collisions_2018-2020.csv`) was extracted from the former project. The Motor Vehicle Collisions crash table contains details on the crash event. Each row represents a crash event. The Motor Vehicle Collisions data tables contain information from all police reported motor vehicle collisions in NYC. The dataset has to be preprocessed again in order to meet the new specifications.\n",
    "\n",
    "The weather dataset (`weather2018.csv`) was already given by the supervisors of the project. It contains the weather conditions of the city of New York during the summer of 2018.\n",
    "\n",
    "The map dataset was obtained from this [cartography web page](https://cartographyvectors.com/map/508-new-york-city-boroughs-ny).\n",
    "\n",
    "The datasets are located in the folder `Data/`. Following are the loading of each dataset and the import of the required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the correct functionality of the executions, the following folders and all their files are needed:\n",
    "\n",
    "- `Data/`: Folder containing the datasets.\n",
    "- `Modules/`: Folder containing the modules used in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install altair==5.1.2 pandas==1.5.3 numpy==1.23.5 altair==5.1.2 h3pandas==0.2.5 geopandas==0.13.2 vegafusion[embed]>=1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point \n",
    "from Modules import preprocessing as pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = './Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing of the files involved a collaborative effort using both OpenRefine and selected Python libraries. This strategic approach was adopted to take advantage of the unique strengths and capabilities offered by each tool. OpenRefine facilitated initial data cleaning and transformation tasks, with its user-friendly interface for effective manipulation of datasets. Simultaneously, Python libraries were utilized to perform more complex data operations and manipulations, with special emphasis on the extensive functionalities and flexibility they provide. This combination allowed for a comprehensive preprocessing workflow that maximized efficiency and accuracy in preparing the data for subsequent analyses and visualization tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial step involved loading the original dataset into a `Pandas` dataframe, primarily to apply a date range filter efficiently. The rationale behind this approach was to optimize the data filtering process, considering the considerable size of the original dataset. The volume of data posed challenges within OpenRefine, leading to slow and inefficient computational processes. By filtering the dataset using `Pandas`, it allowed for a more streamlined and quicker selection of the desired date range. Following this initial filtering phase, the refined dataset was exported as a `.csv` file (`collisions-2018.csv`) and subsequently imported into OpenRefine for further data processing and cleaning procedures. This sequential approach ensured a balance between computational efficiency and data handling capabilities across both Pandas and OpenRefine, resulting in a more effective preprocessing workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRASH DATE</th>\n",
       "      <th>CRASH TIME</th>\n",
       "      <th>BOROUGH</th>\n",
       "      <th>ZIP CODE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>ON STREET NAME</th>\n",
       "      <th>CROSS STREET NAME</th>\n",
       "      <th>OFF STREET NAME</th>\n",
       "      <th>...</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 2</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 3</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 4</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 5</th>\n",
       "      <th>COLLISION_ID</th>\n",
       "      <th>VEHICLE TYPE CODE 1</th>\n",
       "      <th>VEHICLE TYPE CODE 2</th>\n",
       "      <th>VEHICLE TYPE CODE 3</th>\n",
       "      <th>VEHICLE TYPE CODE 4</th>\n",
       "      <th>VEHICLE TYPE CODE 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-06</td>\n",
       "      <td>18:05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.771038</td>\n",
       "      <td>-73.83413</td>\n",
       "      <td>(40.771038, -73.83413)</td>\n",
       "      <td>WHITESTONE EXPRESSWAY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Following Too Closely</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4345591</td>\n",
       "      <td>Station Wagon/Sport Utility Vehicle</td>\n",
       "      <td>Motorcycle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CRASH DATE CRASH TIME BOROUGH  ZIP CODE   LATITUDE  LONGITUDE  \\\n",
       "0  2020-09-06      18:05     NaN       NaN  40.771038  -73.83413   \n",
       "\n",
       "                 LOCATION         ON STREET NAME CROSS STREET NAME  \\\n",
       "0  (40.771038, -73.83413)  WHITESTONE EXPRESSWAY               NaN   \n",
       "\n",
       "  OFF STREET NAME  ...  CONTRIBUTING FACTOR VEHICLE 2  \\\n",
       "0             NaN  ...          Following Too Closely   \n",
       "\n",
       "   CONTRIBUTING FACTOR VEHICLE 3  CONTRIBUTING FACTOR VEHICLE 4  \\\n",
       "0                            NaN                            NaN   \n",
       "\n",
       "   CONTRIBUTING FACTOR VEHICLE 5  COLLISION_ID  \\\n",
       "0                            NaN       4345591   \n",
       "\n",
       "                   VEHICLE TYPE CODE 1  VEHICLE TYPE CODE 2  \\\n",
       "0  Station Wagon/Sport Utility Vehicle           Motorcycle   \n",
       "\n",
       "   VEHICLE TYPE CODE 3 VEHICLE TYPE CODE 4 VEHICLE TYPE CODE 5  \n",
       "0                  NaN                 NaN                 NaN  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions = pd.read_csv(dir + '/collisions_2018-2020.csv')\n",
    "collisions.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115740, 30)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions['YEAR'] = pd.DatetimeIndex(collisions['CRASH DATE']).year\n",
    "collisions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79383, 30)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions = collisions[collisions['YEAR'] == 2018]\n",
    "collisions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Type Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In OpenRefine, the data conversion process involved several attribute adjustments. The CRASH DATE attribute underwent a conversion to a date type for enhanced consistency and data clarity. Meanwhile, both COLLISION ID and CRASH TIME were temporarily set as string types.\n",
    "\n",
    "Attributes pertaining to the geographical location of the collisions were modified to strings, accompanied by specific notations. As part of this process, all values were standardized to uppercase, and any extra spaces were removed where applicable. This standardization was implemented to facilitate the effectiveness of the clustering method utilized for collectively inspecting and modifying cells. The objective was to streamline the identification and correction of any inconsistencies or inaccuracies within the data, ensuring a more uniform and reliable dataset for subsequent analyses.\n",
    "\n",
    "The attributes pertaining to the number of persons involved in the collision underwent a data type conversion to integers within the dataset. This decision was driven by the discrete nature of these values and the expectation that these numerical counts wouldn't contain negative values.\n",
    "\n",
    "Conversely, the attributes related to vehicles and factors involved in the collisions were retained as strings temporarily. This choice was made to maintain flexibility in handling these attributes during subsequent data processing and analysis phases, ensuring that any necessary modifications or categorizations could be applied effectively as the analysis progressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Selection and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previous knowledge of the dataset, a subset of attributes was selected for further analysis. This selection was based on the relevance of the attributes to the research questions and the availability of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_cols = ['CRASH DATE', 'CRASH TIME', 'BOROUGH', 'ZIP CODE', 'LATITUDE', 'LONGITUDE', 'ON STREET NAME', 'OFF STREET NAME', 'NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED', 'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED', 'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED', 'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED', 'CONTRIBUTING FACTOR VEHICLE 1', 'COLLISION_ID', 'VEHICLE TYPE CODE 1']\n",
    "\n",
    "collisions = collisions[interest_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geographic attributes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the previous project, **ON STREET NAME** and **OFF STREET NAME** seem to be the same attribute, but with different names. The web site of the dataset cointains the following descriptions:\n",
    "\n",
    "- **ON STREET NAME**: *Street on which the collision occurred*.\n",
    "- **OFF STREET NAME**: *Street address if known*.\n",
    "\n",
    "This gives the idea that both attributes probably contain the same information. Furthermore, there are no rows with both attributes filled, which makes the idea of merging both attributes plausible and would consolidate information without redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 19)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions[(collisions['ON STREET NAME'].notnull()) & (collisions['OFF STREET NAME'].notnull())].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79157, 19)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions[(collisions['ON STREET NAME'].notnull()) | (collisions['OFF STREET NAME'].notnull())].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting attribute after merging both columns is called **STREET NAME** and contains the street name/address where the collision occurred, with no missing values. Some rows will have a more detailed description of the street, while others will only have the name of the street. The fusion of the two columns will be done in OpenRefine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering process utilized the key collision method in conjunction with the fingerprint keying function, applied separately to each individual geographical attribute. After a couple of iterations, no substantial alterations in attribute values were identified. However, during this process, misspellings were detected and rectified to ensure data accuracy.\n",
    "\n",
    "The misspellings were corrected and the clusterization was done again. The results were the same, which means that the values were already consistent. To verify the result, a Neares Neighbours analysis was done as well but without finding any significant variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vehicle attributes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen already that there are many classes of the VEHICLE CODE TYPE 1 values. To simplify this complexity, the project statement force to reduce the diversity of classes by employing a clustering technique. This clustering methodology allowed us to condense the multitude of classes within VEHICLE CODE TYPE 1 into a more manageable set of clusters, facilitating a more comprehensible and concise representation for subsequent visualization and analysis purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASAR ESTA FUNC A PREPROCESSING\n",
    "\n",
    "collisions['VEHICLE TYPE CODE 1'] = collisions['VEHICLE TYPE CODE 1'].replace(['Taxi', 'TAXI'], 'TAXI')\n",
    "collisions['VEHICLE TYPE CODE 1'] = collisions['VEHICLE TYPE CODE 1'].replace(['Fire', 'FD tr', 'firet', 'fire', 'FIRE', 'fd tr', 'FD TR', 'FIRE', 'FIRET'], 'FIRE')\n",
    "collisions['VEHICLE TYPE CODE 1'] = collisions['VEHICLE TYPE CODE 1'].replace(['AMBUL', 'Ambulance', 'ambul', 'AMB', 'Ambul', 'AMBULANCE', 'AMBU'], 'AMBULANCE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting classes are the following:\n",
    "\n",
    "- **TAXI**: \n",
    "- **FIRE**:\n",
    "- **AMBULANCE**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar strategy was done with the CONTRIBUTING FACTOR VEHICLE 1 attribute. However, the aggregation was not so exhaustive since this attribute wasn't needed a priori for the main questions that the visualizations should answer. For this attribute basic merge transformations were applied in OpenRefine until no \"strange\" or \"uninformative\" nor repeated classes remained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of persons attributes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the visualization purposes, the differentantion of **PERSONS**, **PEDESTRIANS**, **CYCLISTS** and **MOTORISTS** (**INJURED/KILLED**) is irrelevant. A more useful attribute would be the total number of persons involved in the collision. This can be obtained by summing the four attributes under the assumption that the **PERSONS** attribute is not the sum of the other three attributes.\n",
    "\n",
    "This condition was needed to be checked because the documentation of the dataset was not precise enough to determinate if **NUMBER OF PERSON INJURED/KILLED** was an aggregate from the other three columns or not.\n",
    "\n",
    "*Note: The metadata information available in the web of the dataset was: \"Number of persons injured/killed\" regarding the **NUMBER OF PERSONS INJURED/KILLED**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions['NUMBER OF PERSONS INJURED'].equals(collisions['NUMBER OF PEDESTRIANS INJURED'] + collisions['NUMBER OF CYCLIST INJURED'] + collisions['NUMBER OF MOTORIST INJURED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions['NUMBER OF PERSONS INJURED'].equals(collisions['NUMBER OF PEDESTRIANS INJURED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions['NUMBER OF PERSONS KILLED'].equals(collisions['NUMBER OF PEDESTRIANS KILLED'] + collisions['NUMBER OF CYCLIST KILLED'] + collisions['NUMBER OF MOTORIST KILLED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions['NUMBER OF PERSONS KILLED'].equals(collisions['NUMBER OF PEDESTRIANS KILLED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen by the logical comprobations, the **NUMBER OF PERSONS INJURED/KILLED** is not the sum of the other three attributes. Furthermore, the terms persons and pedestrians are not equal, as one could have thought that the term persons was used to refer to pedestrians.\n",
    "\n",
    "Based on this, the discrete attributes refering to the injured people were summed to obtain **NUMBER OF INJURED** and the discrete attributes refering to the killed people were summed to obtain **NUMBER OF KILLED**. The **NUMBER OF INJURED/KILLED** attributes were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASAR ESTA FUNC A PREPROCESAMIENTO\n",
    "\n",
    "collisions['TOTAL INJURED'] = collisions.filter(regex='INJURED').sum(axis=1)\n",
    "collisions['TOTAL KILLED'] = collisions.filter(regex='KILLED').sum(axis=1)\n",
    "\n",
    "collisions = collisions.drop(collisions.iloc[:, 8:16], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions.to_csv(dir + '/collisions-2018.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions = pd.read_csv(dir + '/collisions-2018_prepro_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESTO SE HACE PORQ EL PROFE SE EQUIVOCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TAXI', 'AMBULANCE', 'FIRE'], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions = collisions[collisions['VEHICLE TYPE CODE 1'].isin(['FIRE', 'TAXI', 'AMBULANCE'])]\n",
    "\n",
    "collisions['VEHICLE TYPE CODE 1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4092, 12)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the dataset contains the attributes needed (with the weather attributes as an exception) for the analysis and some extra attributes that were considered interesting for some possible extra analysis or insights that we could think about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has already been mentioned the existence of some missing values. In the previous section, the verification of missing values was done with the .isnull() method of Pandas. However, this method does not take into account the `NaN` values. In order to check the existence of NaN values, the .isna() method was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: bool)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp = (collisions.isnull().sum() == collisions.isna().sum())\n",
    "comp[comp == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen previously, all the missing values of the dataset are detected both with .isnull() and .isna(). After this check, notice that the only attributes with missign values are corresponding to geographical properties of the collisions. We can apply a similar strategy as the one used in the previous project to fill the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COLLISION_ID                        0\n",
       "CRASH DATE                          0\n",
       "CRASH TIME                          0\n",
       "BOROUGH                          1385\n",
       "ZIP CODE                         1385\n",
       "LATITUDE                          292\n",
       "LONGITUDE                         292\n",
       "STREET NAME                         0\n",
       "CONTRIBUTING FACTOR VEHICLE 1       0\n",
       "VEHICLE TYPE CODE 1                 0\n",
       "TOTAL INJURED                       0\n",
       "TOTAL KILLED                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions['STREET NAME'] = collisions['STREET NAME'].apply(pre.capitalize_street)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, our focus lies in addressing the missing attributes within the coordinates. Our approach involves leveraging the information present in the **STREET NAME** column, which is devoid of any missing values. The outlined process is as follows:\n",
    "\n",
    "Utilization of the `Nominatim` geocoding service enables us to retrieve coordinates corresponding to the street names. Whenever available, we employ the BOROUGH or ZIP CODES to refine our search parameters. Upon a successful search, we populate the missing values with the obtained coordinates. However, in cases where the search yields no results, we maintain the missing values in their current state. This is done by the `pre.fill_missing_coordinates()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dir + '/collisions-2018_prepro_v2.csv'):\n",
    "    collisions = collisions.apply(pre.fill_missing_coordinates, axis=1)\n",
    "    collisions.to_csv(dir + '/collisions-2018_prepro_v2.csv', index=False)\n",
    "else:\n",
    "    collisions = pd.read_csv(dir + '/collisions-2018_prepro_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COLLISION_ID                        0\n",
       "CRASH DATE                          0\n",
       "CRASH TIME                          0\n",
       "BOROUGH                          1385\n",
       "ZIP CODE                         1385\n",
       "LATITUDE                           34\n",
       "LONGITUDE                          34\n",
       "STREET NAME                         0\n",
       "CONTRIBUTING FACTOR VEHICLE 1       0\n",
       "VEHICLE TYPE CODE 1                 0\n",
       "TOTAL INJURED                       0\n",
       "TOTAL KILLED                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83088954056696"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "34*100/collisions.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportion of missing values reduces notably for the coordinate values, accounting for only $0.83%$ of the total rows. Addressing the missing entries in the **BOROUGH** and **ZIP CODE** columns involves a strategy that involves verifying whether a given point, defined by its coordinates (**LONGITUDE**, **LATITUDE**), resides within the boundary polygon of distinct **BOROUGH** or **ZIP CODE** values. When the original attribute value is null, this methodology endeavors to allocate the appropriate borough based on the geographic coordinates provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that, the extraction of polygons for both the **BOROUGH** and **ZIP CODE** attributes is imperative. The polygons for the **BOROUGH** attribute were acquired from the existing files accessible within the `./Data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_poly = pre.get_borough_polygons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_poly = pre.get_zip_polygons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jpniz\\OneDrive\\Documentos\\UPC\\Q5\\VI\\NYC-Collisions-Visualization-Project\\2-Interactive-Dashboard\\Modules\\preprocessing.py:196: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '11219' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[idx, 'ZIP CODE'] = z\n",
      "c:\\Users\\jpniz\\OneDrive\\Documentos\\UPC\\Q5\\.conda\\Lib\\site-packages\\shapely\\predicates.py:946: RuntimeWarning: invalid value encountered in within\n",
      "  return lib.within(a, b, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "pre.fill_missing_borough_zip(collisions, borough_poly, zip_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BOROUGH\n",
       "MANHATTAN        2365\n",
       "BROOKLYN          617\n",
       "QUEENS            568\n",
       "BRONX             470\n",
       "STATEN ISLAND      12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions['BOROUGH'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this breakdown, it's evident that Manhattan has the highest count of imputed values, followed by Brooklyn, Queens, Bronx, and finally Staten Island with the lowest count. This distribution suggests that a significant portion of missing values were inferred and filled with these borough names. The disparities in counts might be attributed to various factors such as the availability of geographic data or the frequency of missing values per borough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COLLISION_ID                      0\n",
       "CRASH DATE                        0\n",
       "CRASH TIME                        0\n",
       "BOROUGH                          60\n",
       "ZIP CODE                         77\n",
       "LATITUDE                         34\n",
       "LONGITUDE                        34\n",
       "STREET NAME                       0\n",
       "CONTRIBUTING FACTOR VEHICLE 1     0\n",
       "VEHICLE TYPE CODE 1               0\n",
       "TOTAL INJURED                     0\n",
       "TOTAL KILLED                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A noticeable reduction in the count of missing values is apparent in the **BOROUGH** and **ZIP CODE** attributes compared to the missing values found in the coordinates. The relatively diminished count of missing values in these attributes allows for their removal from the dataset without impacting the analytical outcomes. This removal is feasible because these missing values represent a small proportion and their exclusion does not significantly affect the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions.isnull().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions = collisions.dropna()\n",
    "collisions['BOROUGH'] = collisions['BOROUGH'].apply(pre.capitalize_boroughs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions.to_csv(f'{dir}/collisions_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>datetime</th>\n",
       "      <th>tempmax</th>\n",
       "      <th>tempmin</th>\n",
       "      <th>temp</th>\n",
       "      <th>feelslikemax</th>\n",
       "      <th>feelslikemin</th>\n",
       "      <th>feelslike</th>\n",
       "      <th>dew</th>\n",
       "      <th>humidity</th>\n",
       "      <th>...</th>\n",
       "      <th>solarenergy</th>\n",
       "      <th>uvindex</th>\n",
       "      <th>severerisk</th>\n",
       "      <th>sunrise</th>\n",
       "      <th>sunset</th>\n",
       "      <th>moonphase</th>\n",
       "      <th>conditions</th>\n",
       "      <th>description</th>\n",
       "      <th>icon</th>\n",
       "      <th>stations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new york</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>26.7</td>\n",
       "      <td>17.9</td>\n",
       "      <td>21.6</td>\n",
       "      <td>26.7</td>\n",
       "      <td>17.9</td>\n",
       "      <td>21.6</td>\n",
       "      <td>19.2</td>\n",
       "      <td>86.8</td>\n",
       "      <td>...</td>\n",
       "      <td>16.5</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-06-01T05:27:08</td>\n",
       "      <td>2018-06-01T20:21:01</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Rain, Partially cloudy</td>\n",
       "      <td>Partly cloudy throughout the day with rain in ...</td>\n",
       "      <td>rain</td>\n",
       "      <td>72505394728,72055399999,KLGA,KJRB,KNYC,F1417,7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       name    datetime  tempmax  tempmin  temp  feelslikemax  feelslikemin  \\\n",
       "0  new york  2018-06-01     26.7     17.9  21.6          26.7          17.9   \n",
       "\n",
       "   feelslike   dew  humidity  ...  solarenergy  uvindex  severerisk  \\\n",
       "0       21.6  19.2      86.8  ...         16.5        9         NaN   \n",
       "\n",
       "               sunrise               sunset  moonphase  \\\n",
       "0  2018-06-01T05:27:08  2018-06-01T20:21:01        0.6   \n",
       "\n",
       "               conditions                                        description  \\\n",
       "0  Rain, Partially cloudy  Partly cloudy throughout the day with rain in ...   \n",
       "\n",
       "   icon                                           stations  \n",
       "0  rain  72505394728,72055399999,KLGA,KJRB,KNYC,F1417,7...  \n",
       "\n",
       "[1 rows x 33 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather = pd.read_csv(f'{dir}/weather2018.csv')\n",
    "weather.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122, 33)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tempmax</th>\n",
       "      <th>tempmin</th>\n",
       "      <th>temp</th>\n",
       "      <th>feelslikemax</th>\n",
       "      <th>feelslikemin</th>\n",
       "      <th>feelslike</th>\n",
       "      <th>dew</th>\n",
       "      <th>humidity</th>\n",
       "      <th>precip</th>\n",
       "      <th>precipprob</th>\n",
       "      <th>...</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>winddir</th>\n",
       "      <th>sealevelpressure</th>\n",
       "      <th>cloudcover</th>\n",
       "      <th>visibility</th>\n",
       "      <th>solarradiation</th>\n",
       "      <th>solarenergy</th>\n",
       "      <th>uvindex</th>\n",
       "      <th>severerisk</th>\n",
       "      <th>moonphase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>122.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>122.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>28.092623</td>\n",
       "      <td>20.943443</td>\n",
       "      <td>24.152459</td>\n",
       "      <td>29.482787</td>\n",
       "      <td>20.982787</td>\n",
       "      <td>24.762295</td>\n",
       "      <td>18.202459</td>\n",
       "      <td>71.332787</td>\n",
       "      <td>6.372730</td>\n",
       "      <td>61.475410</td>\n",
       "      <td>...</td>\n",
       "      <td>19.313115</td>\n",
       "      <td>170.000820</td>\n",
       "      <td>1017.756557</td>\n",
       "      <td>45.837705</td>\n",
       "      <td>15.398361</td>\n",
       "      <td>197.408197</td>\n",
       "      <td>17.027049</td>\n",
       "      <td>7.049180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.483443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.994318</td>\n",
       "      <td>3.134558</td>\n",
       "      <td>3.273566</td>\n",
       "      <td>5.509071</td>\n",
       "      <td>3.221562</td>\n",
       "      <td>4.025217</td>\n",
       "      <td>3.797247</td>\n",
       "      <td>11.060401</td>\n",
       "      <td>29.415435</td>\n",
       "      <td>48.866019</td>\n",
       "      <td>...</td>\n",
       "      <td>6.794662</td>\n",
       "      <td>98.724638</td>\n",
       "      <td>5.093360</td>\n",
       "      <td>33.667473</td>\n",
       "      <td>1.071338</td>\n",
       "      <td>84.628818</td>\n",
       "      <td>7.304544</td>\n",
       "      <td>2.535215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.286131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.700000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>18.700000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>15.200000</td>\n",
       "      <td>1005.600000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25.075000</td>\n",
       "      <td>18.900000</td>\n",
       "      <td>22.075000</td>\n",
       "      <td>25.075000</td>\n",
       "      <td>18.900000</td>\n",
       "      <td>22.075000</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>63.325000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>13.800000</td>\n",
       "      <td>65.425000</td>\n",
       "      <td>1014.125000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.325000</td>\n",
       "      <td>138.425000</td>\n",
       "      <td>11.875000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.700000</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>24.300000</td>\n",
       "      <td>28.950000</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>18.850000</td>\n",
       "      <td>72.800000</td>\n",
       "      <td>0.241500</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>180.150000</td>\n",
       "      <td>1018.100000</td>\n",
       "      <td>41.900000</td>\n",
       "      <td>15.900000</td>\n",
       "      <td>209.450000</td>\n",
       "      <td>18.050000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>31.175000</td>\n",
       "      <td>23.300000</td>\n",
       "      <td>26.450000</td>\n",
       "      <td>33.800000</td>\n",
       "      <td>23.300000</td>\n",
       "      <td>27.375000</td>\n",
       "      <td>21.575000</td>\n",
       "      <td>79.150000</td>\n",
       "      <td>2.255000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>24.100000</td>\n",
       "      <td>253.275000</td>\n",
       "      <td>1021.475000</td>\n",
       "      <td>75.075000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>262.775000</td>\n",
       "      <td>22.775000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>35.600000</td>\n",
       "      <td>26.700000</td>\n",
       "      <td>30.600000</td>\n",
       "      <td>41.400000</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>34.600000</td>\n",
       "      <td>23.600000</td>\n",
       "      <td>93.400000</td>\n",
       "      <td>231.468000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>40.700000</td>\n",
       "      <td>350.100000</td>\n",
       "      <td>1030.200000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>331.300000</td>\n",
       "      <td>28.600000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          tempmax     tempmin        temp  feelslikemax  feelslikemin  \\\n",
       "count  122.000000  122.000000  122.000000    122.000000    122.000000   \n",
       "mean    28.092623   20.943443   24.152459     29.482787     20.982787   \n",
       "std      3.994318    3.134558    3.273566      5.509071      3.221562   \n",
       "min     18.700000   11.700000   16.800000     18.700000     11.700000   \n",
       "25%     25.075000   18.900000   22.075000     25.075000     18.900000   \n",
       "50%     28.700000   21.100000   24.300000     28.950000     21.100000   \n",
       "75%     31.175000   23.300000   26.450000     33.800000     23.300000   \n",
       "max     35.600000   26.700000   30.600000     41.400000     29.500000   \n",
       "\n",
       "        feelslike         dew    humidity      precip  precipprob  ...  \\\n",
       "count  122.000000  122.000000  122.000000  122.000000  122.000000  ...   \n",
       "mean    24.762295   18.202459   71.332787    6.372730   61.475410  ...   \n",
       "std      4.025217    3.797247   11.060401   29.415435   48.866019  ...   \n",
       "min     16.800000   10.300000   46.000000    0.000000    0.000000  ...   \n",
       "25%     22.075000   15.100000   63.325000    0.000000    0.000000  ...   \n",
       "50%     24.400000   18.850000   72.800000    0.241500  100.000000  ...   \n",
       "75%     27.375000   21.575000   79.150000    2.255000  100.000000  ...   \n",
       "max     34.600000   23.600000   93.400000  231.468000  100.000000  ...   \n",
       "\n",
       "        windspeed     winddir  sealevelpressure  cloudcover  visibility  \\\n",
       "count  122.000000  122.000000        122.000000  122.000000  122.000000   \n",
       "mean    19.313115  170.000820       1017.756557   45.837705   15.398361   \n",
       "std      6.794662   98.724638          5.093360   33.667473    1.071338   \n",
       "min      8.800000   15.200000       1005.600000    0.200000   11.000000   \n",
       "25%     13.800000   65.425000       1014.125000   15.000000   15.325000   \n",
       "50%     18.400000  180.150000       1018.100000   41.900000   15.900000   \n",
       "75%     24.100000  253.275000       1021.475000   75.075000   16.000000   \n",
       "max     40.700000  350.100000       1030.200000  100.000000   16.000000   \n",
       "\n",
       "       solarradiation  solarenergy     uvindex  severerisk   moonphase  \n",
       "count      122.000000   122.000000  122.000000         0.0  122.000000  \n",
       "mean       197.408197    17.027049    7.049180         NaN    0.483443  \n",
       "std         84.628818     7.304544    2.535215         NaN    0.286131  \n",
       "min         19.500000     1.600000    1.000000         NaN    0.000000  \n",
       "25%        138.425000    11.875000    6.000000         NaN    0.250000  \n",
       "50%        209.450000    18.050000    8.000000         NaN    0.500000  \n",
       "75%        262.775000    22.775000    9.000000         NaN    0.720000  \n",
       "max        331.300000    28.600000   10.000000         NaN    0.970000  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name                  0\n",
       "datetime              0\n",
       "tempmax               0\n",
       "tempmin               0\n",
       "temp                  0\n",
       "feelslikemax          0\n",
       "feelslikemin          0\n",
       "feelslike             0\n",
       "dew                   0\n",
       "humidity              0\n",
       "precip                0\n",
       "precipprob            0\n",
       "precipcover           0\n",
       "preciptype           47\n",
       "snow                  0\n",
       "snowdepth             0\n",
       "windgust             29\n",
       "windspeed             0\n",
       "winddir               0\n",
       "sealevelpressure      0\n",
       "cloudcover            0\n",
       "visibility            0\n",
       "solarradiation        0\n",
       "solarenergy           0\n",
       "uvindex               0\n",
       "severerisk          122\n",
       "sunrise               0\n",
       "sunset                0\n",
       "moonphase             0\n",
       "conditions            0\n",
       "description           0\n",
       "icon                  0\n",
       "stations              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.isnull().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
