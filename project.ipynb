{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Authors**:\n",
    "-  *Juan P. Zaldivar E.*\n",
    "-  *Enrique Mill√°n X.*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the preprocessing required for the datasets. The datasets are:\n",
    "- Colissions dataset.\n",
    "- Weather dataset.\n",
    "- New York Map.\n",
    "\n",
    "\n",
    "<!-- explicar que primer se hace una exploracion visual y luego el preprocesing? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset obtention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [*collision*](https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95) dataset was already given by the instructors of the project.\n",
    "\n",
    "The *weather* dataset was obtained following the next steps:\n",
    "\n",
    "- Visit the [NOAA Climate Data Online Search](https://www.ncdc.noaa.gov/cdo-web/search) web page.\n",
    "\n",
    "- Select the following options:\n",
    "  - `Weather Observation Type/Dataset -> Daily Summaries, Date Range -> 2018-01-01 to 2020-12-31, Search For -> Cities, Search Term -> New York City.`\n",
    "\n",
    "- Look for \"*New York, NY US*\" and click in ADD TO CART. Now, click the cart in the top right corner.\n",
    "\n",
    "- Select \"*Custom GHCN-Daily CSV*\", and the date previously selected (2018-01-01 to 2020-12-31). We are selecting more information than needed (to avoid disjoint downloads), but we will later filter it with ``Pandas`` and ``Open Refine``. Click continue.\n",
    "\n",
    "- Fill the three options, and select \"*metric units*\".\n",
    "\n",
    "- Fill all the options remaining and click continue. There are some options that will be probably not needed, but we will further analyze this when cleaning the datasets.\n",
    "\n",
    "- Type the email where you want to receive the data so the order can start.\n",
    "\n",
    "The *map* dataset was obtained following the next steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are located in the folder `Data/` and the results are saved in the folder `Data/Preprocessed/`. Following are the loading of each dataset and the import of the required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import h3pandas as h3\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from Modules import preprocessing as pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# comentar las versiones de los paquetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing of the files was done conjointly with OpenRefine and the proposed python libraries in order to be able to take advantages of both tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dir = './Data'\n",
    "collision_exists = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collision dataset preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration?\n",
    "\n",
    "The collision table incorporates information pertaining to individual crash events, with each row representing a distinct collision incident. The dataframe compile details from all police reported motor vehicle collisions in NYC. \n",
    "\n",
    "<!-- ...se hace una breve descripcion de la exploracion? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset was first loaded into a ``Pandas`` dataframe in order to filter the desired range of dates. The reason lays in a more efficient way to filter the data, taking the size of the original dataset into account. This volumn of data made the computational process in OpenRefine very slow and ineffcient. After this initial filtering, the dataset was exported to a ``.csv`` file and loaded into OpenRefine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'{dir}/collisions_2018-2020.csv'):\n",
    "    collision = pd.read_csv(f'{dir}/collisions_2018-2020.csv')\n",
    "    colission_exists = True\n",
    "else:\n",
    "    collision = pd.read_csv(f'{dir}/collisions.csv')\n",
    "\n",
    "collision.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the filtered version does not exist, we proceed with the filtering. To filter the data to the summer of 2018 and 2020, the first step is to change the data type of the **CRASH DATE** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not colission_exists:\n",
    "    collision = pre.time_filter(collision, 'CRASH DATE')\n",
    "    collision.to_csv(f'{dir}/collisions_2018-2020.csv', index=False)\n",
    "\n",
    "collision.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the filtering, the dataset was exported to a ``.csv`` file and loaded into OpenRefine. The procedure and reasoning taken in OpenRefine will be explained and justified in the present section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data type conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data conversion was done in OpenRefine. The **CRASH DATE** attribute was converted to a date type. The **COLLISION ID** and **CRASH TIME** were set as strings for the time being.\n",
    "\n",
    " The attributes relating the greographic location of the collision were set as strings with some special remarks. All the values were set to uppercase and the extra spaces were removed (if any). The reason for this was to ease the work of the clusterization method used to collectively inspect and edit cells in the case that there were some values that were not correctly or consistently written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the attributes refering to the number of persons involved in the collision, the data type was set to integer. The reason for this is that the values are discrete and the values are not expected to be negative.\n",
    "\n",
    "The vehicle and factors attributes were left as strings for the time being. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data selection and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the following transformations were applied with OpenRefine, but some comprobations to justify the transformations are carried on in this section of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision = pd.read_csv(f'{dir}/collisions_2018-2020_prepro_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision[\"CRASH DATE\"] = pd.to_datetime(precollision[\"CRASH DATE\"])\n",
    "precollision['YEAR'] = precollision['CRASH DATE'].astype(str).str[:4]\n",
    "precollision[\"DAY NAME\"] = precollision[\"CRASH DATE\"].dt.day_name()\n",
    "precollision[\"TYPE OF DAY\"] = np.where(precollision[\"DAY NAME\"].isin([\"Saturday\", \"Sunday\"]), \"Weekend\", \"Weekday\")\n",
    "\n",
    "for _ in range(3):\n",
    "    cols = precollision.columns.tolist()\n",
    "    cols = cols[:1] + cols[-1:] + cols[1:-1]\n",
    "    precollision = precollision[cols]\n",
    "\n",
    "precollision.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision['CRASH TIME'] = pd.to_datetime(precollision['CRASH TIME'], format='%H:%M').dt.time\n",
    "precollision['CRASH TIME INTERVAL'] = precollision['CRASH TIME'].apply(lambda x: f\"{x.hour:02}\")\n",
    "precollision.drop(columns=['CRASH TIME'], inplace=True)\n",
    "\n",
    "precollision['CRASH MOMENT'] = precollision['CRASH TIME INTERVAL'].apply(pre.categorize_moment)\n",
    "\n",
    "# move TIME INTERVAL to the fourth column\n",
    "cols = precollision.columns.tolist()\n",
    "cols = cols[:3] + cols[-1:] + cols[3:-1]\n",
    "precollision = precollision[cols]\n",
    "\n",
    "# move CRASH MOMENT to the fifth column\n",
    "cols = precollision.columns.tolist()\n",
    "cols = cols[:4] + cols[-1:] + cols[4:-1]\n",
    "precollision = precollision[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geographical attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, **ON STREET NAME** and **OFF STREET NAME** seem to be the same attribute, but with different names. The web site of the dataset cointains the following descriptions:\n",
    "\n",
    "- **ON STREET NAME**: *Street on which the collision occurred*.\n",
    "- **OFF STREET NAME**: *Street address if known*.\n",
    "\n",
    "This gives the idea that both attributes probably contain the same information. Furthermore, there are no rows with both attributes filled, which makes the idea of merging both attributes plausible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision[(collision['ON STREET NAME'].notnull()) & (collision['OFF STREET NAME'].notnull())].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision[(collision['ON STREET NAME'].notnull()) | (collision['OFF STREET NAME'].notnull())].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting attribute after merging both columns is called **STREET NAME** and contains the street name/address where the collision occurred, with no missing values. Some rows will have a more detailed description of the street, while others will only have the name of the street. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CROSS STREET**, which is the third attribute related to the street enviroment, and represents the name of the closest street crossing the street of the crash, can be dropped since it is not useful for the intended analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, **LOCATION** seems to contain the tuple (**LATITUDE**, **LONGITUDE**), so we could, a priori, remove the two extra attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision[(collision['LOCATION'].notnull()) & (collision['LATITUDE'].notnull()) & (collision['LONGITUDE'].notnull())].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows where the three attributes are not missing does not cover the total number of rows, but there are no rows where the **LOCATION** attribute is missing and at least one of the other two attributes is not missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision[(collision['LOCATION'].isnull()) & (collision['LATITUDE'].isnull()) & (collision['LONGITUDE'].notnull()) | (collision['LATITUDE'].notnull()) & (collision['LONGITUDE'].isnull())].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision[(collision['LOCATION'].isnull())].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which makes the rest of the rows (7667) with missing values in the three attributes. This means that the **LATITUDE** and **LONGITUDE** attributes can be removed, since the **LOCATION** attribute contains the same information. With this transformation, the number of attributes is reduced by two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusterization was done with the *key collision* method and the *fingerprint* keying function. No significant variations were found after a copuble iterations in the values of the attributes but misspellings were found. The misspellings were corrected and the clusterization was done again. The results were the same, which means that the values were already consistent. To verify the result, a Neares Neighbours analysis was done as well but without finding any significant variation.\n",
    "\n",
    "--> mas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vehicle attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding vehicle information, the statement of the project specifies that only the **VEHICLE CODE TYPE 1** is of interest to the visualization, so all the other vehicle codes can be removed (2-5). With this, the cotributing factors of the other vehicles can also be removed.\n",
    "\n",
    "We have seen already that there are many classes of the **VEHICLE CODE TYPE 1** values, so we reduced the number of classes by clusterization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(collision['VEHICLE TYPE CODE 1'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the clusterization (key collision and fingerprint keying function) of the **VEHICLE TYPE CODE 1** attribute we found a lot of misspellings and inconsistencies. The clusterization was done iteratively, correcting the misspellings and inconsistencies found in each iteration. After a couple of iterations, where a lot of subclasses were merged into the same type of vehicle thus aggregating more the data, the number of unique clasess was reduced to the point where the last subclasses manually could be merged manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision = pre.cluterize_vehicle_type(precollision, 'VEHICLE TYPE CODE 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting classes of the **VEHICLE CODE TYPE 1** attribute are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(precollision['VEHICLE TYPE CODE 1'].unique()), precollision['VEHICLE TYPE CODE 1'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This types of vehicles were selected so there wasn't too much nor too many classes and the user could still get an insight of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision['VEHICLE TYPE CODE 1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar strategy was done with the **CONTRIBUTING FACTOR VEHICLE 1** attribute. However, the aggrgation was not so exhaustive since this attribute wasn't needed a priori for the main questions that the visualizations should answer. For this attribute basic merge transformations were applied in OpenRefine until no \"strange\" or \"uninformative\" nor repeated classes remained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(precollision['CONTRIBUTING FACTOR VEHICLE 1'].unique()), precollision['CONTRIBUTING FACTOR VEHICLE 1'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of persons attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the visualization purposes, the differentantion of **PERSONS**, **PEDESTRIANS**, **CYCLISTS** and **MOTORISTS** (**INJURED/KILLED**) is irrelevant. A more useful attribute would be the total number of persons involved in the collision. This can be obtained by summing the four attributes under the assumption that the **PERSONS** attribute is not the sum of the other three attributes. \n",
    "\n",
    "This condition was needed to be checked because the documentation of the dataset was not precise enough to determinate if **NUMBER OF PERSON INJURED/KILLED** was an aggregate from the other three columns or not.\n",
    "\n",
    "*Note: The metadata information available in the web of the dataset was: \"Number of persons injured/killed\" regarding the **NUMBER OF PERSONS INJURED/KILLED**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision['NUMBER OF PERSONS INJURED'].equals(collision['NUMBER OF PEDESTRIANS INJURED'] + collision['NUMBER OF CYCLIST INJURED'] + collision['NUMBER OF MOTORIST INJURED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision['NUMBER OF PERSONS INJURED'].equals(collision['NUMBER OF PEDESTRIANS INJURED'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen by the logical comprobations, the **NUMBER OF PERSONS INJURED/KILLED** is not the sum of the other three attributes. Furthermore, the terms persons and pedestrians are not equal, as one could have thought that the term persons was used to refer to pedestrians.\n",
    "\n",
    "Based on this, the discrete attributes refering to the injured people were summed to obtain **NUMBER OF INJURED** and the discrete attributes refering to the killed people were summed to obtain **NUMBER OF KILLED**. The **NUMBER OF INJURED/KILLED** attributes were removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenRefine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the dataset contains the attributes needed (with the *weather* attributes as an exception) for the analysis and some extra attributes that were considered interesting for some possible extra analysis or insights that we could think about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has already been mentioned the existence of some missing values. In the previous section, the verification of missing values was done with the ``.isnull()`` method of ``Pandas``. However, this method does not take into account the ``NaN`` values. In order to check the existence of ``NaN`` values, the ``.isna()`` method was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = (precollision.isnull().sum() == precollision.isna().sum())\n",
    "comp[comp == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen previously, all the missing values of the dataset are detected both with ``.isnull()`` and ``.isna()``. After this check, we could group the attributes with missign values in three separeted clusters:\n",
    "- Geographical attributes\n",
    "- Injured/Killed attributes\n",
    "- Vehicle attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation of geographic attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first cluster is formed with reference to the geographicals attributes. The attributes in this cluster are:\n",
    "- **BOROUGH**\n",
    "- **ZIP CODE**\n",
    "- **LOCATION**\n",
    "- **STREET NAME**\n",
    "- **CROSS STREET NAME**\n",
    "\n",
    "Notice that the attributes with the less missing values is **STREET NAME** with only a $0.20\\%$ of the entire dataset, partially thanks to the merge of **ON STREET** and **OFF STREET** attributes in the previous sections. We used this information to impute the missing coordinates of the **LOCATION** attribute and the rest of the attributes sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision['LOCATION'].isnull().sum(), precollision['STREET NAME'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a longitude and latitude column from the coordinates column\n",
    "precollision[['LATITUDE', 'LONGITUDE']] = precollision['LOCATION'].str.split(', ', expand=True)\n",
    "precollision['LATITUDE'] = precollision['LATITUDE'].str.replace('(', '')\n",
    "precollision['LONGITUDE'] = precollision['LONGITUDE'].str.replace(')', '')\n",
    "\n",
    "# convert the columns to float\n",
    "precollision['LATITUDE'] = precollision['LATITUDE'].astype(float)\n",
    "precollision['LONGITUDE'] = precollision['LONGITUDE'].astype(float)\n",
    "\n",
    "precollision.drop(columns=['LOCATION'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_map = gpd.read_file('Data/new-york-city-boroughs-ny_.geojson')\n",
    "\n",
    "boroughs = ['Bronx', 'Brooklyn', 'Manhattan', 'Queens', 'Staten Island']\n",
    "\n",
    "borough_polly = {}\n",
    "for b in boroughs:\n",
    "    poly = nyc_map[nyc_map['name'] == b]['geometry']\n",
    "    borough_polly[b] = poly.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'{dir}/precollision_2018-2020_prepro_v2.csv'):\n",
    "    for idx, row in precollision.iterrows():\n",
    "        if row['BOROUGH'] is not None:\n",
    "            lon = row['LONGITUDE']\n",
    "            lat = row['LATITUDE']\n",
    "\n",
    "            if lat is not None and lon is not None:\n",
    "                p = Point(lon, lat)\n",
    "                for b, poly in borough_polly.items():\n",
    "                    if p.within(poly):\n",
    "                        precollision.loc[idx, 'BOROUGH'] = b.upper()\n",
    "                        break\n",
    "    \n",
    "    precollision.to_csv(f'{dir}/precollision_2018-2020_prepro_v2.csv', index=False)\n",
    "else:\n",
    "    precollision = pd.read_csv(f'{dir}/precollision_2018-2020_prepro_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision['ZIP CODE'].isnull().sum(), precollision['BOROUGH'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPLICAR EL PORQ HAY MENOS MISSING VALUES EN BOROUGH QUE EN LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision[(precollision['LATITUDE'].isna()) & (precollision['BOROUGH'].isna())].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation of vehicle attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poner unespecified en factor $i$ si el vehiculo $i$ es no null y factor $i$ es null\n",
    "# o\n",
    "# poner unknown en vehiculo $i$ si el vehiculo $i$ es null y factor $i$ no es null\n",
    "\n",
    "# son disjuntas estas operaciones?????????????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some rows of the dataset, the **CONTRIBUTING FACTOR VEHICLE** is missing but the **VEHICLE TYPE CODE** is not. This suggests that the vehicle type is known, but the factor that contributed to the collision is not. In order to fill this missing values, the factor was set as *unespecified*. This was done for all the rows and columns where the **CONTRIBUTING FACTOR VEHICLE** was missing with the above condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.imputation_with_ref_col(precollision, 'CONTRIBUTING FACTOR VEHICLE 1', 'VEHICLE TYPE CODE 1', 'Unspecified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, in some rows of the dataset, the **VEHICLE TYPE CODE** is missing but the **CONTRIBUTING FACTOR VEHICLE** is not. This suggests that the factor that contributed to the collision is known, but the vehicle type is not. In order to fill this missing values, the vehicle type was set as *unknown*. This was done for all the rows and columns where the **VEHICLE TYPE CODE** was missing with the above condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.imputation_with_ref_col(precollision, 'VEHICLE TYPE CODE 1', 'CONTRIBUTING FACTOR VEHICLE 1', 'UNKNOWN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- comentar la reduccion de missing values -->\n",
    "\n",
    "Notice that the only missing values in the **CONTRIBUTING FACTOR VEHICLE** attribute and **VEHICLE TYPE CODE** attribute are in the same rows. This means that the number of missing values in the **CONTRIBUTING FACTOR VEHICLE** attribute and **VEHICLE TYPE CODE** attribute is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# como se hace para imputar los missing de los dos campos a la vez?\n",
    "\n",
    "# code 1 como unespecified/unknown\n",
    "# code resto no existent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation of number of person attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# como se hace para imputar los missing del numero de personas? o mejor se elimina?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision[precollision['NUMBER OF INJURED'].isnull() | precollision['NUMBER OF KILLED'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the resulting rows with missing values of the number of persons involved in the collision are a small fraction of the total dataset, we decided to set them to 0, assuming that there were no persons involved in the collision.\n",
    "\n",
    "Given the small percentage of the rows $0.01\\%$, it was considered that there would not be a significant impact in the final visualization whether the missing values were set to 0 or the rows with missing values were dropped. However, we decided to set the missing values to 0 in order to keep the rows and not lose some of its information. This was done also because **NUMBER OF INJURED** and  **NUMBER OF KILLED** were attributes that are not necessary for the main visualizations and we were only keeping them in case we found an intereseting extra visualization with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision['NUMBER OF INJURED'].fillna(0, inplace=True)\n",
    "precollision['NUMBER OF KILLED'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this done, the only meaningful missing values are the 226 of **STREET NAME**, that is because the other location related attributes could be mostly imputed with **STREET NAME**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision.to_csv(f'{dir}/collisions_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather dataset preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the process of downloading the data, we selected all the attributes available. Now, we will explore the data and select the ones that are useful for our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv(f'{dir}/weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only interested in the rows where the date is inside the timeranges of 01/06/2018 - 31/09/2018 and 01/06/2020 - 31/09/2020, we will filter the data to only include those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pre.time_filter(weather, 'DATE')\n",
    "weather.to_csv(f'{dir}/weather_2018-2020.csv', index=False)\n",
    "weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the documentation of the weather datatset and the attributes present, each row represents some selected observations (values) available for a given **STATION** and **DATE**. Neither the **STATION** nor the **DATE** are unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weather['STATION'].unique()), len(weather['DATE'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the first 6 columns (**SATTION**, **NAME**, **LATITUDE**, **LONGITUDE**, **ELEVATION** and **DATE**), the rest of the attributes correspond to optional flags and their respective attributes (definded by the weather documentation *Note: The 4 flags listed [...] are optional on the Custom GHCN-Daily ASCII Form*) and therefore can contain several null values. That is the reason for the large quantity of sparse attributes detected. We will explore the data to see which attributes are useful for our purpose.\n",
    "\n",
    "All these atributes correspond to the Table 4. A brief description is collected in the following table:\n",
    "\n",
    "| Attribute | Description |\n",
    "| --- | --- |\n",
    "| *PRCP* | Precipitation (mm) |\n",
    "| *SNOW* | Snowfall (mm) |\n",
    "| *SNWD* | Snow depth (mm) |\n",
    "| *TSUN* | Daily total sunshine (minutes) |\n",
    "| *TMAX* | Maximum temperature (Celsius) |\n",
    "| *TMIN* | Minimum temperature (Celsius) |\n",
    "| *TAVG* | Average temperature (Celsius) |\n",
    "| *TOBS* | Temperature at the time of observation |\n",
    "| *AWND* | Average daily wind speed (meters per second) |\n",
    "| *WT** | Weather type * |\n",
    "\n",
    "\n",
    "The next table contains the attributes that we considered that are not useful for the visualization purpose:\n",
    "\n",
    "| Attribute | Description |\n",
    "| --- | --- |\n",
    "| *PGTM* | Peak gust time (hours and minutes, i.e., HHMM) |\n",
    "| *DARP* | Number of days included in the multiday precipitation total (MDPR) |\n",
    "| *DASF* | Number of days included in the multiday snowfall total (MDSF) |\n",
    "| *MDPR* | Multiday precipitation total (mm; use with DAPR and DWPR, if available) |\n",
    "| *MDSF* | Multiday snowfall total (mm; use with DASF and DWSF, if available) |\n",
    "| *PSUN* | Daily percent of possible sunshine (percent; use with TSUN, if available) |\n",
    "| *WSF** | Fastest *-minute wind speed (meters per second) |\n",
    "| *WDF** | Direction of fastest *-second wind (degrees) |\n",
    "| *WESD* | Water equivalent of snow on the ground (decimal mm) |\n",
    "| *WESF* | Water equivalent of snowfall (decimal mm) |\n",
    "\n",
    "\n",
    "Both **DARP** and **DASF** don't directly measure specific weather conditions. They are more of an aggregate measure of precipitation/snowfall over multiple days, which is not useful for our purpose. The same applies to **MDPR**, **MDSF** and **PSUN** that are also not useful for our purpose since they are not specific weather conditions.\n",
    "Other attributes such as **WDF**, **WSF**, **WESD**, **WESF** or **PGTM** (among others) give data that is diffuclt to collect, therefore having a lot of null values, and is too specific to be useful for our analysis. For instance, all wind related attributes are not really useful for our analysis, and a simple and general attribute such as **AWND** is much more useful for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **\"MEASURE\"_ATTRIBUTES** columns refer to the rest of the tables in the documentation (Table 1, 2, 3). These tables serve as flags for the measurement, quality and source of the data respectively. The values available for these attributes cover a wide range. In the data selection process we will analyze the subset of those values that are present in the data and discuss if they are useful for our purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geographical attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we will start by joining **LATITUDE** and **LONGITUDE** in a **LOCATION** attribute following the format of the *collision* dataset. Since both attributes ahave no missing values, the resulting column will not have any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather[(weather['LATITUDE'].notnull()) & (weather['LONGITUDE'].notnull())].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['LOCATION'] = '(' + weather['LATITUDE'].astype(str) + ', ' + weather['LONGITUDE'].astype(str) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will delete the **ELEVATION** column since it can't be related in any way with the previous dataset and the information it gives is not useful for our visualization purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.drop(columns=['LATITUDE', 'LONGITUDE', 'ELEVATION'], inplace=True)\n",
    "\n",
    "cols = list(weather.columns)\n",
    "cols = [cols[-1]] + cols[:-1]\n",
    "weather = weather[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have *LOCATION*, we don't need *STATION* code or *NAME*.\n",
    "\n",
    "-- yo no los quitaria aun, porque si queremos hacer un mapa de estaciones, necesitamos el codigo de estacion y el nombre de la estacion. Si no, no sabemos a que estacion corresponde cada punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.drop(columns=['STATION', 'NAME'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observational attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are analyzing the summer periods of 2018 and 2020, it is highly unlikely to have observations of snow in the records. Under this assumption we can drop the **SNOW** and **SNWD** attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['SNOW'].unique(), weather['SNWD'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we supposed, the only observations registred of **SNOW** and **SNWD** are null values or 0. Therefore, we can drop these columns and their respective flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.drop(columns=['SNOW', 'SNWD', 'SNOW_ATTRIBUTES', 'SNWD_ATTRIBUTES'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data exploration done in the previous section, we can remove the following attributes and their respective flags since we concluded their utility is really limited for our purpose: **DARP**, **DASF**, **MDPR**, **MDSF**, **PSUN**, **WDF**, **WESD**, **WESF** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.drop(columns=['DAPR',\n",
    "                        'DASF',\n",
    "                        'PGTM',\n",
    "                        'MDPR',\n",
    "                        'MDSF',\n",
    "                        'PSUN',\n",
    "                        'WDF2',\n",
    "                        'WDF5',\n",
    "                        'WESD',\n",
    "                        'WESF',\n",
    "                        'WSF2',\n",
    "                        'WSF5',\n",
    "                        'DAPR_ATTRIBUTES',\n",
    "                        'DASF_ATTRIBUTES',\n",
    "                        'MDPR_ATTRIBUTES',\n",
    "                        'MDSF_ATTRIBUTES',\n",
    "                        'PSUN_ATTRIBUTES',\n",
    "                        'WDF2_ATTRIBUTES',\n",
    "                        'WDF5_ATTRIBUTES',\n",
    "                        'WESD_ATTRIBUTES',\n",
    "                        'WESF_ATTRIBUTES',\n",
    "                        'WSF2_ATTRIBUTES',\n",
    "                        'WSF5_ATTRIBUTES'\n",
    "                        ], inplace=True)\n",
    "\n",
    "weather.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the **WT*** columns, they all describe a specific weather condition that could be referred as *adverse condition* given the descriptions from the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Attribute | Description |\n",
    "| --- | --- |\n",
    "| *WT02* | Heavy fog or heaving freezing fog (not always distinguished from fog) |\n",
    "| *WT03* | Thunder |\n",
    "| *WT04* | Ice pellets, sleet, snow pellets, or small hail\" |\n",
    "| *WT05* | Hail (may include small hail) |\n",
    "| *WT06* | Glaze or rime |\n",
    "| *WT08* | Smoke or haze |\n",
    "| *WT09* | Blowing or drifting snow |\n",
    "| *WT11* | High or damaging winds |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As checked, they all have the null values in the same rows, which could indicate that the ``nan`` value could mean a ``False`` value for all the adverse type of weather conditions, that is the corresponding record refers to a *normal* weather day. Therefore, we will replace the ``nan`` values with ``0``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather[weather['WT01'].isnull() & weather['WT02'].isnull() & weather['WT03'].isnull() & weather['WT04'].isnull() & weather['WT05'].isnull() & weather['WT06'].isnull() & weather['WT08'].isnull() & weather['WT09'].isnull() & weather['WT11'].isnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['WT01'].unique(), weather['WT02'].unique(), weather['WT03'].unique(), weather['WT04'].unique(), weather['WT05'].unique(), weather['WT06'].unique(), weather['WT08'].unique(), weather['WT09'].unique(), weather['WT11'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this knowledge, the final decision was to merge all the **WT*** columns into a single one called **ADVERSE CONDITION**, which would have boolean values (0 or 1) indicating if the day was normal or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_columns = ['WT01', 'WT02', 'WT03', 'WT04', 'WT05', 'WT06', 'WT08', 'WT09', 'WT11']\n",
    "\n",
    "weather['ADVERSE_CONDITIONS'] = weather[wt_columns].isnull().all(axis=1).astype(int)\n",
    "weather['ADVERSE_CONDITIONS'] = 1 - weather['ADVERSE_CONDITIONS']\n",
    "\n",
    "weather['ADVERSE_CONDITIONS'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flag attributes of the **WT*** columns will be removed given that the weather conditions columns were aggregated into a single column and thus the flags are not meaningful anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_columns_att = [col + '_ATTRIBUTES' for col in wt_columns]\n",
    "\n",
    "weather.drop(columns=wt_columns, inplace=True)\n",
    "weather.drop(columns=wt_columns_att, inplace=True)\n",
    "\n",
    "weather.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flags attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As done in the previous section, the ***_ATTRIBUTES** contain specific and irrelevant information  for our task, so it was decided to remove them all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = weather.columns.tolist()\n",
    "\n",
    "for col in cols:\n",
    "    if '_ATTRIBUTES' in col:\n",
    "        print(col, weather[col].unique(), sep=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_columns_att = [col for col in weather.columns if '_ATTRIBUTES' in col]\n",
    "\n",
    "weather.drop(columns=wt_columns_att, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the sparseness described in the data exploration, it is logical to check for columns that contain only or mainly missing values in order to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, **TSUN** column contains only missing values. Therefore, we will remove the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.drop(columns=['TSUN'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For **PRCP**, we will assume that the null values are 0, that is if there's no record of precipitation (the value is null), it's because there were no precipitations. We make this assumption because the number of null values represents around 10% of the dataset so it's not such a risk to make this assumption (and even less risk taking into account that the data is from Summer, when it's less likely to rain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['PRCP'].fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding temperature, the great number of missing values is a problem. We will impute the missing values with the avergae temperature per day of each attribute (*TMAX*, *TMIN*, *TOBS*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['TMAX'] = weather.groupby('DATE')['TMAX'].transform(lambda x: x.fillna(x.mean()))\n",
    "weather['TMIN'] = weather.groupby('DATE')['TMIN'].transform(lambda x: x.fillna(x.mean()))\n",
    "weather['TOBS'] = weather.groupby('DATE')['TOBS'].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the three attributes from above have been imputed, and **TAVG** contains the most missing values, it was decided to remove this row. In case we need an average for temperature we can compute it with the previous attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.drop(columns=['TAVG'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the same strategy on AWND to get rid of the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['AWND'] = weather.groupby('DATE')['AWND'].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For possible future analysis we will categorize the **AWND** and **PRCP** with the following scales:\n",
    "\n",
    "- **AWND** with Beaufort scale (https://en.wikipedia.org/wiki/Beaufort_scale)\n",
    "\n",
    "- **PRCP**  following the classification of the World Meteorological Organization, 2018(https://www.researchgate.net/figure/Rain-intensity-classifications-according-to-the-World-Meteorological-Organization-2018_tbl1_353769617)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_aggregated = weather.copy()\n",
    "weather_aggregated.drop(columns=['LOCATION', 'ADVERSE_CONDITIONS'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_aggregated = weather_aggregated.groupby('DATE').mean().reset_index()\n",
    "weather_aggregated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def beaufort_scale(wind_speed_mps):\n",
    "#     if wind_speed_mps < 0.3:\n",
    "#         return \"Calm\"\n",
    "#     elif wind_speed_mps < 1.5:\n",
    "#         return \"Light Air\"\n",
    "#     elif wind_speed_mps < 3.4:\n",
    "#         return \"Light Breeze\"\n",
    "#     elif wind_speed_mps < 5.5:\n",
    "#         return \"Gentle Breeze\"\n",
    "#     elif wind_speed_mps < 8.0:\n",
    "#         return \"Moderate Breeze\"\n",
    "#     elif wind_speed_mps < 10.8:\n",
    "#         return \"Fresh Breeze\"\n",
    "#     elif wind_speed_mps < 13.9:\n",
    "#         return \"Strong Breeze\"\n",
    "#     elif wind_speed_mps < 17.2:\n",
    "#         return \"Near Gale\"\n",
    "#     elif wind_speed_mps < 20.8:\n",
    "#         return \"Gale\"\n",
    "#     elif wind_speed_mps < 24.5:\n",
    "#         return \"Strong Gale\"\n",
    "#     elif wind_speed_mps < 28.5:\n",
    "#         return \"Storm\"\n",
    "#     else:\n",
    "#         return \"Hurricane\"\n",
    "    \n",
    "\n",
    "# weather_aggregated['BEAUFORT_SCALE'] = weather_aggregated['AWND'].apply(beaufort_scale)\n",
    "\n",
    "# weather_aggregated['BEAUFORT_SCALE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the Beaufort scale categories\n",
    "# beaufort_scale = [0.5, 1.5, 3.3, 5.5, 7.9, 10.7, 13.8, 17.1, 20.7, 24.4, 28.4, 32.6]\n",
    "\n",
    "# # Categorize the 'AWND' column by the Beaufort scale\n",
    "# weather['AWND_category'] = pd.cut(weather['AWND'], beaufort_scale, labels=range(0, 11), include_lowest=True)\n",
    "\n",
    "# # Convert the 'AWND_category' column to integer type\n",
    "# # weather['AWND_category'] = weather['AWND_category'].astype(int)\n",
    "\n",
    "# # Print the first 5 rows of the updated dataframe\n",
    "# weather['AWND_category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rain_intensity_scale(prcp_mm):\n",
    "    if prcp_mm < 2.5*24:\n",
    "        return \"Slight\"\n",
    "    elif prcp_mm < 10*24:\n",
    "        return \"Moderate\"\n",
    "    elif prcp_mm < 50*24:\n",
    "        return \"Heavy\"\n",
    "    else:\n",
    "        return \"Violent\"\n",
    "    \n",
    "weather_aggregated['PRCP_SCALE'] = weather_aggregated['PRCP'].apply(rain_intensity_scale)\n",
    "\n",
    "weather_aggregated['PRCP_SCALE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_aggregated['MEAN_TEMP'] = weather_aggregated[['TMAX', 'TMIN', 'TOBS']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessed data saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.to_csv(f'{dir}/weather_clean.csv', index=False)\n",
    "weather_aggregated.to_csv(f'{dir}/weather_aggregated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization creation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv(f'{dir}/weather_clean.csv')\n",
    "collisions = pd.read_csv(f'{dir}/collisions_clean.csv')\n",
    "nyc_map = gpd.read_file(f'{dir}/new-york-city-boroughs-ny_.geojson')\n",
    "weather_aggregated = pd.read_csv(f'{dir}/weather_aggregated.csv')\n",
    "\n",
    "\n",
    "collisions['YEAR'] = collisions['YEAR'].astype(str)\n",
    "\n",
    "alt.data_transformers.enable(\"vegafusion\")\n",
    "\n",
    "colores_hex = [\n",
    "    '#a3ffd6',  # Verde agua intenso\n",
    "    '#d69bf5',  # P√∫rpura intenso\n",
    "    '#ff8080',  # Rojo intenso\n",
    "    '#80ff80',  # Verde intenso\n",
    "    '#80bfff',  # Azul intenso\n",
    "    '#ffff66',  # Amarillo intenso\n",
    "    '#ffcc66',  # Naranja intenso\n",
    "    '#c9cba3',  # P√∫rpura intenso\n",
    "    '#66cccc',  # Verde azulado intenso\n",
    "    '#ff66b3',  # Rosa intenso\n",
    "    '#ffb056',  # Verde claro intenso\n",
    "    '#98c1d9',  # Rojo anaranjado intenso\n",
    "    '#ffafcc'   # Verde intenso\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions[['YEAR', 'NUMBER OF INJURED']].groupby(['YEAR']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "34832.0*100/44459.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "228*100/177"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are accidents more frequent during weekdays or weekends? Is there any difference between before COVID-19 and after?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the question in a pretty straightforward way, we decided that a slope chart would be a really good option since we first intend to show the difference between the average of weekdays and weekends for both years, so using a barchart would probably not be optimal. We consider that the results allow to answer the question pretty easily, however we want to explore some options that give the user more detail showing, for instance, all the days of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions['TYPE OF DAY'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = collisions.loc[:, ['YEAR', 'DAY NAME', 'TYPE OF DAY']]\n",
    "df.insert(0, 'COUNT', 1)\n",
    "\n",
    "df = df.groupby(['YEAR', 'TYPE OF DAY']).count().reset_index()\n",
    "# divide count by 5 if it is a weekday\n",
    "df['COUNT'] = df.apply(lambda x: x['COUNT']/5 if x['TYPE OF DAY'] == 'Weekday' else x['COUNT']/2, axis=1)\n",
    "\n",
    "\n",
    "slope = alt.Chart(df).mark_line().encode(\n",
    "    x=alt.X('YEAR:N', title='Year', axis=alt.Axis(labelAngle=0)),\n",
    "    y=alt.Y('COUNT:Q', title='Collisions per Day'),\n",
    "    color=alt.Color('TYPE OF DAY:N', legend=alt.Legend(title='Day Type'))\n",
    ")\n",
    "\n",
    "pts = alt.Chart(df).mark_point(\n",
    "    filled=True,\n",
    "    opacity=1\n",
    ").encode(\n",
    "    x=alt.X('YEAR:N', title='Year', axis=alt.Axis(labelAngle=0)),\n",
    "    y=alt.Y('COUNT:Q', title='Collisions per Day'),\n",
    "    color=alt.Color('TYPE OF DAY:N',\n",
    "                    scale=alt.Scale(range=['#B3E9C7', '#8367C7']),\n",
    "                    legend=None)\n",
    ")\n",
    "alt.layer(slope, pts).properties(width=100, height=300, title='Collisions per Day Type by Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = collisions.groupby('YEAR')[['NUMBER OF INJURED']].sum().reset_index()\n",
    "\n",
    "df2 = collisions.groupby('YEAR')[['COLLISION_ID']].count()\n",
    "df = df.merge(df2, on='YEAR')\n",
    "\n",
    "df['TOTAL PER 10'] = 10 * df['NUMBER OF INJURED'] / df['COLLISION_ID']\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car = (\"M640 320V368C640 385.7 625.7 400 608 400H574.7C567.1 445.4 527.6 480 480 480C432.4 480 392.9 445.4 385.3 400H254.7C247.1 445.4 207.6 480 160 480C112.4 480 72.94 445.4 65.33 400H32C14.33 400 0 385.7 0 368V256C0 228.9 16.81 205.8 40.56 196.4L82.2 92.35C96.78 55.9 132.1 32 171.3 32H353.2C382.4 32 409.1 45.26 428.2 68.03L528.2 193C591.2 200.1 640 254.8 640 319.1V320zM171.3 96C158.2 96 146.5 103.1 141.6 116.1L111.3 192H224V96H171.3zM272 192H445.4L378.2 108C372.2 100.4 362.1 96 353.2 96H272V192zM525.3 400C527 394.1 528 389.6 528 384C528 357.5 506.5 336 480 336C453.5 336 432 357.5 432 384C432 389.6 432.1 394.1 434.7 400C441.3 418.6 459.1 432 480 432C500.9 432 518.7 418.6 525.3 400zM205.3 400C207 394.1 208 389.6 208 384C208 357.5 186.5 336 160 336C133.5 336 112 357.5 112 384C112 389.6 112.1 394.1 114.7 400C121.3 418.6 139.1 432 160 432C180.9 432 198.7 418.6 205.3 400z\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([dict(id=i) for i in range(1, 11)])\n",
    "\n",
    "data['color'] = ['kill/injured' if i < 6 else 'non' for i in range(1, 11)]\n",
    "\n",
    "data['year'] = ['2018'] * 10\n",
    "\n",
    "alt.Chart(data).transform_calculate(\n",
    "    row=\"ceil(datum.id/10)\"\n",
    ").transform_calculate(\n",
    "    col=\"datum.id - datum.row*10\"\n",
    ").mark_point(\n",
    "    filled=True,\n",
    "    size=0.04\n",
    ").encode(\n",
    "    alt.X(\"col:O\", axis=None),\n",
    "    alt.Y(\"row:O\", axis=None),\n",
    "    alt.ShapeValue(car),\n",
    "    color=alt.Color('color:N',\n",
    "                    scale=alt.Scale(domain=['kill/injured', 'non'],range=['#5603ad', '#9BA8C7']),\n",
    "                    legend=None)\n",
    ").properties(\n",
    "    width=900,\n",
    "    height=130\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([dict(id=i) for i in range(1, 11)])\n",
    "\n",
    "data['color'] = ['kill/injured' if i < 10 else 'non' for i in range(1, 11)]\n",
    "\n",
    "data['year'] = ['2020'] * 10\n",
    "\n",
    "alt.Chart(data).transform_calculate(\n",
    "    row=\"ceil(datum.id/10)\"\n",
    ").transform_calculate(\n",
    "    col=\"datum.id - datum.row*10\"\n",
    ").mark_point(\n",
    "    filled=True,\n",
    "    size=0.04\n",
    ").encode(\n",
    "    alt.X(\"col:O\", axis=None),\n",
    "    alt.Y(\"row:O\", axis=None),\n",
    "    alt.ShapeValue(car),\n",
    "    color=alt.Color('color:N',\n",
    "                    scale=alt.Scale(domain=['kill/injured', 'non'],range=['#5603ad', '#9BA8C7']),\n",
    "                    legend=None)\n",
    ").properties(\n",
    "    width=900,\n",
    "    height=130\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and simpler option to give more detail than in the previous chart is to make a barplot. With just to options it wasn't such a good idea, but if we show all the days of the week it makes much more sense. Since we want the names of the days to be legible the idea is to make an horizontal bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = collisions[['DAY NAME']]\n",
    "\n",
    "alt.Chart(df).mark_bar().encode(\n",
    "    x=alt.X('count():Q',\n",
    "            title='Number of Collisions'),\n",
    "    y=alt.Y('DAY NAME:O',\n",
    "            sort=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "            title='Day of the Week'),\n",
    "    color=alt.Color('DAY NAME:O',\n",
    "                    scale=alt.Scale(domain=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "                                    range=['#B3E9C7', '#B3E9C7', '#B3E9C7', '#B3E9C7', '#B3E9C7', '#8367C7', '#8367C7']),\n",
    "                                    legend=None)\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=200,\n",
    "    title='Number of Collisions by Day of the Week'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, this option is simple but effective in its task. However, we are also asked to show if there is a difference between before and after COVID-19 (that is between 2018 and 2020), so in future iterations of the visualization we should take this into account.\n",
    "\n",
    "The next visualizaiton intends to answer the same question, but also tries to show if there is a time period of the day (morning, afternoon, night) that has more crashes than others. Nevertheless, the outcome is not really useful since the comparison between bars and inside bars is pretty diffuclt, moreover, the colors catch the attention so it's also more difficult (it takes longer since there are more variables encoded) than in the previous visualization to identify the days with more crashes. Again, it fails to show a differentiation between years so we shall look for other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = collisions[['DAY NAME', 'CRASH MOMENT']]\n",
    "\n",
    "bars = alt.Chart(df).mark_bar().transform_calculate(\n",
    "    order=\"{'Morning': 1, 'Afternoon': 2, 'Night': 3}[datum['CRASH MOMENT']]\"\n",
    ").encode(\n",
    "    x=alt.X('count():Q',\n",
    "            title='Number of Collisions',\n",
    "            stack='zero'),\n",
    "    y=alt.Y('DAY NAME:N',\n",
    "            sort=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "            title='Day of the Week'),\n",
    "    color=alt.Color('CRASH MOMENT:N',\n",
    "                    title='Moment of the Day',\n",
    "                    scale=alt.Scale(domain=['Morning', 'Afternoon', 'Night'],\n",
    "                                    range=['#B3E9C7', '#9BA8C7', '#8367C7'])),\n",
    "    order=alt.Order('order:O')\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=200,\n",
    "    title='Number of Collisions by Day of the Week'\n",
    ")\n",
    "\n",
    "text = alt.Chart(collisions[['DAY NAME', 'CRASH MOMENT']]).mark_text(\n",
    "    align='right',\n",
    "    color='black',\n",
    "    baseline='middle'\n",
    ").transform_calculate(\n",
    "    order=\"{'Morning': 1, 'Afternoon': 2, 'Night': 3}[datum['CRASH MOMENT']]\"\n",
    ").encode(\n",
    "   x=alt.X('count():Q',\n",
    "            title='Number of Collisions',\n",
    "            stack='zero'),\n",
    "    y=alt.Y('DAY NAME:N',\n",
    "            sort=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "            title='Day of the Week'),\n",
    "    detail=alt.Detail('CRASH MOMENT:N'),\n",
    "    text=alt.Text('count():Q'),\n",
    "    order=alt.Order('order:O')\n",
    ")\n",
    "\n",
    "bars + text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with the differentiation between 2018 and 2020, in which the previous visualizations fail, an interesting option is to do a paired horizontal bar chart. Since there is only two classes inside every row, (theoretically) the comaprison between instances from different rows is not as difficult as it would be if there were more categories. Therefore, we consider this visualization to accomplish its objectives, but we will keep iterating to see if a better result can be found. Therefore, we consider that this visualization should accomplish its objectives and be useful t answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = collisions[['YEAR', 'DAY NAME']]\n",
    "\n",
    "alt.Chart(df).transform_aggregate(\n",
    "    count='count()',\n",
    "    groupby=['DAY NAME', 'YEAR']\n",
    ").mark_bar().encode(\n",
    "    x=alt.X('count:Q',\n",
    "            title='Number of Collisions'),\n",
    "    y=alt.Y('YEAR:O',\n",
    "            axis=alt.Axis(grid=False, ticks=False, labels=False),\n",
    "            title=''),\n",
    "    color=alt.Color('YEAR:N',\n",
    "                    scale=alt.Scale(domain=['2018', '2020'],\n",
    "                                    range=['#B3E9C7', '#8367C7']),\n",
    "                    legend=alt.Legend(title='Year')),\n",
    "    row=alt.Row('DAY NAME:O',\n",
    "                      header=alt.Header(title='Day of the Week'),\n",
    "                      sort=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "                      title='Day of the Week')\n",
    ").properties(\n",
    "    title='Number of Collisions by Day of the Week and Year'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to what we thought, the result is not as good as expected. Because of the separation per days, the comparison between days is not so easy, and this problem is accentuated for the 2020 instances which vary less and because of this and the separation the differences are more difficult to notice. Moreover, the names of the days are in vertical which makes them diffuclt to read; this problem can be solved easily but teh previously mentioned ones will still be there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to ease the comparison between instances, an option we considered useful was a small multiples of the initial barchart, one for each year. At first this may not seem a great option if we are trying to enhance the comparison, but we consider that the fact of only havig two barcharts that share indexes and are aligned makes the comparison between instances of the different charts pretty easy and, of course, the comparison between instances of the same chart is as good as for the initial plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = collisions[['YEAR', 'DAY NAME']]\n",
    "\n",
    "alt.Chart(df).mark_bar().encode(\n",
    "    x=alt.X('count():Q'),\n",
    "    y=alt.Y('DAY NAME:O',\n",
    "            title='Day of the Week',\n",
    "            sort=alt.SortField(field='DAY NAME', order='ascending')),\n",
    "    color=alt.Color('YEAR:O',\n",
    "                    scale=alt.Scale(domain=['2018', '2020'], range=['#C2F8CB', '#8367C7']),\n",
    "                    legend=None),\n",
    "    column=alt.Column('YEAR:O', title='Year')\n",
    ").properties(\n",
    "    width=200,\n",
    "    height=200,\n",
    "    title='Number of Collisions by Day of the Week (2018 vs. 2020)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main flaw of this plot is that it may be \"wasting some space\" and it may not be the best option to see the general trend. \n",
    "\n",
    "Since we will be showing more detail of the days of the week in the visualization for the third question, we finally decided to use the slope chart for this question. As mentioned earlier, we consider that it is really useful to answer the question posed. Moreover it is simple and does not occupy as much space as the other options. The main flaw of this plot, even if the question doesn't ask explicitly for it, is that some detail is lost when aggregating all days in weekday or weekend. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there any type of vehicle more prone to participate in accidents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions['YEAR'] = collisions['YEAR'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions[\"VEHICLE TYPE CODE 1\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we aggregated the type of vehicles to a total of just 13 classes, the bar chart is the first option that comes to our mind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = collisions[['VEHICLE TYPE CODE 1']]\n",
    "\n",
    "alt.Chart(df).mark_bar().encode(\n",
    "    x=alt.X('VEHICLE TYPE CODE 1:N', title='Vehicle Type', axis=alt.Axis(labelAngle=-45)),\n",
    "    y=alt.Y('count():Q', title='Number of Collisions')\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=200,\n",
    "    title='Number of Collisions by Vehicle Type'\n",
    ").configure_mark(\n",
    "    color='lightblue'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is decent but at the same time there are some classes barely noticeable. We want to clarify that the answer of this question is impossible with the available data, since we would need the traffic percenatge or proportion of every type of vehicle to determine if there is a vehicle more prone to have crashes than others. This happens because, for example, there are much more cars than ambulances, so the number of total car crashes is much bigger than the numbe of total ambulance crashes, and without the traffic proportions we can't really say if one of the vehicles is more prone to have an accident than the others. \n",
    "\n",
    "Having stated this, we thought that polar area charts could be a good option to make the classes with less crashes more noticeable. We are aware that the comparison between areas is more difficult, however we believe that the main (erroneous for the reasons previously explained) conclusions for the question are still clear enough and pretty understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(collisions).encode(\n",
    "    alt.Theta(\"VEHICLE TYPE CODE 1:N\",\n",
    "              stack = True,\n",
    "              sort=alt.EncodingSortField(field=\"count\", op=\"count\", order='descending')),\n",
    "    alt.Radius(\"count()\",\n",
    "               scale=alt.Scale(type=\"sqrt\", zero=True, rangeMin=20)),\n",
    "    color=alt.Color(\"VEHICLE TYPE CODE 1:N\",\n",
    "                    sort=alt.EncodingSortField(field=\"count\", op=\"count\", order='descending'),\n",
    "                    scale=alt.Scale(range=colores_hex),\n",
    "                    legend=alt.Legend(title=\"Vehicle Type\")),\n",
    ").mark_arc(\n",
    "    innerRadius=5, stroke=\"#fff\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of this palette with no colors being extremely similar or much more appealing than others allows for a good idenftification of the thirteen instances. The strength of the chart is taht allows to see the groups with less total crashes, so the user can compare the smaller classes aswell as the bigger ones. However, the problem is, as explained previously, that areas are much more difficult to compare than lenghts, so although the user can say which are bigger than others (since its ordered by size), he can't really quantify how much bigger are ones from others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this problem, we determined that the solution would be to aggregate the numbers with the actual counts of crashes to each of the instances. Which resulted in the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = alt.Chart(collisions).encode(\n",
    "    alt.Theta(\"VEHICLE TYPE CODE 1:N\",\n",
    "              stack = True,\n",
    "              sort=alt.EncodingSortField(field=\"count\", op=\"count\", order='descending')),\n",
    "    alt.Radius(\"count()\",\n",
    "               scale=alt.Scale(type=\"sqrt\", zero=True, rangeMin=20)),\n",
    "    color=alt.Color(\"VEHICLE TYPE CODE 1:N\",\n",
    "                    sort=alt.EncodingSortField(field=\"count\", op=\"count\", order='descending'),\n",
    "                    scale=alt.Scale(range=colores_hex),\n",
    "                    legend=alt.Legend(title=\"Vehicle Type\")),\n",
    ").mark_arc(\n",
    "    innerRadius=5, stroke=\"#fff\"\n",
    ")\n",
    "\n",
    "text = c1.mark_text(\n",
    "    align='center',\n",
    "    baseline='middle',\n",
    "    radiusOffset=20,\n",
    "    fontSize=12.5,\n",
    ").encode(\n",
    "    text='count()'\n",
    ")\n",
    "\n",
    "alt.layer(c1 + text).properties(\n",
    "    title='Number of Collisions by Vehicle Type'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column that sums NUMBER OF PERSONS INJURED and NUMBER OF PERSONS KILLED\n",
    "collisions['TOTAL INJURED/KILLED'] = collisions['NUMBER OF INJURED'] + collisions['NUMBER OF KILLED']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last option we considered is a lollipop chart since it allows an easy comparison of several instances. The result is the following and it is similar to the one of a typical barchart. Simple and effective, but with the same problem of some types of vehicle being unnoticeable and impossible to differentiate (the ones with the lowest counts of crashes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = collisions[['VEHICLE TYPE CODE 1', 'TOTAL INJURED/KILLED']]\n",
    "\n",
    "df1 = df.groupby('VEHICLE TYPE CODE 1').sum(\"TOTAL INJURED/KILLED\").reset_index()\n",
    "df2 = df.groupby('VEHICLE TYPE CODE 1').count().reset_index()\n",
    "\n",
    "df2.columns = ['VEHICLE TYPE CODE 1', 'TOTAL COLLISIONS']\n",
    "\n",
    "df = pd.merge(df1, df2, on='VEHICLE TYPE CODE 1')\n",
    "\n",
    "\n",
    "lolli = alt.Chart(df).mark_bar(\n",
    "    size=3\n",
    ").encode(\n",
    "    x=alt.X('TOTAL COLLISIONS:Q',\n",
    "            title='Total Collisions'),\n",
    "    y=alt.Y('VEHICLE TYPE CODE 1:N',\n",
    "            title='Vehicle Type',\n",
    "            sort=alt.EncodingSortField(field=\"TOTAL COLLISIONS\", order=\"descending\"),\n",
    "            axis=alt.Axis(labelAngle=0, grid=True)),\n",
    "    color=alt.Color('VEHICLE TYPE CODE 1:N',\n",
    "                    title='Vehicle Type',\n",
    "                    legend=None),\n",
    ")\n",
    "\n",
    "pop = alt.Chart(df).mark_circle(\n",
    "    tooltip=True,\n",
    "    size=80,\n",
    "    opacity=1\n",
    ").encode(\n",
    "    x=alt.X('TOTAL COLLISIONS:Q',\n",
    "            title='Total Collisions'),\n",
    "    y=alt.Y('VEHICLE TYPE CODE 1:N',\n",
    "            title='Vehicle Type',\n",
    "            sort=alt.EncodingSortField(\n",
    "                field=\"TOTAL COLLISIONS\",\n",
    "                order=\"descending\"),\n",
    "            axis=alt.Axis(labelAngle=0, grid=True)),\n",
    "    color=alt.Color('VEHICLE TYPE CODE 1:N',\n",
    "                    title='Vehicle Type',\n",
    "                    legend=None),\n",
    "    tooltip=['VEHICLE TYPE CODE 1:N', 'TOTAL COLLISIONS:Q', 'TOTAL INJURED/KILLED:Q']\n",
    ").properties(\n",
    "    title='Lollipop Plot of Collisions by Vehicle Type and Contributing Factor'\n",
    ")\n",
    "\n",
    "lolli + pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided that the best option in this case is the last iteration of the polar area chart. It solves the problem of less common classes being really difficult to differentiate and offers the actual count values so that the user can get an idea of how much difference is between the number of accidents of the different vehicle types. A possible problem is that for this type of chart so many instances could be difficult to interpret at first glance, but since we have chosen a proper palette, ordered the instances by value and written their values, we consider this is just a minor problem that should not affect much the aim of the visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At what time of the day are accidents more common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions['YEAR'] = collisions['CRASH DATE'].astype(str).str[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we aggregated the count accidents by time of the day in hours, we consider that a line chart is a great option to represent the evolution of total car crashes per hours. Also, this type of chart allows to separate the data by years (2018, 2020) very easlily to add extra information. As we can see the result clearly shows the top hours with more crashes and allows an easy comparison between hours and years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = collisions[['COLLISION_ID', 'CRASH TIME INTERVAL', 'YEAR']]\n",
    "\n",
    "alt.Chart(df).mark_area(\n",
    "    point=True,\n",
    "    fillOpacity=0.8,\n",
    "    line=True,\n",
    "    tooltip=True\n",
    "    # interpolate='monotone'\n",
    ").transform_aggregate(\n",
    "    count='count()',\n",
    "    groupby=['YEAR', 'CRASH TIME INTERVAL']\n",
    ").encode(\n",
    "    x=alt.X('CRASH TIME INTERVAL:O', title='Hour of the Day', axis=alt.Axis(labelAngle=0)),\n",
    "    y=alt.Y('count:Q', title='Number of Collisions', stack=None),\n",
    "    color=alt.Color('YEAR:N', title='Year', scale=alt.Scale(domain=['2018', '2020'], range=['lightblue', 'lightgreen'])),\n",
    "    tooltip=['count:Q', 'CRASH TIME INTERVAL:O']\n",
    ").properties(\n",
    "    title='Number of Collisions by Hour of the Day'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option we considered is a heatmap. This time we also add the day of the week and not only the time of the day, so this visualization not only helps knowing in which time of the day there are more crashes but also allows us to answer the question that asks us to analyze if there are more crashes in weekends or in weekdays, giving more detail and context to the first question. We consider the result to be pretty useful and clear. The only issue is that this visualization does not allow to compare between years, however this is not an important problem since, as mentioned, there is a specific visualization for that task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = alt.Chart(collisions[['CRASH TIME INTERVAL', 'DAY NAME', 'YEAR']]).mark_rect(\n",
    "    tooltip=True\n",
    ").encode(\n",
    "    x=alt.X('CRASH TIME INTERVAL:N',\n",
    "            title='Hour of the Day',\n",
    "            axis=alt.Axis(labelAngle=0)),\n",
    "    y=alt.Y('DAY NAME:N',\n",
    "            sort=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "            title='Day of the Week'),\n",
    "    color=alt.Color('count():Q',\n",
    "                    title='Number of Collisions',\n",
    "                    scale=alt.Scale(range=['#f0fff1', '#5603ad'])),\n",
    "    tooltip=['count()']\n",
    ").properties(\n",
    "    title='Number of Collisions by Hour of the Day and Day of the Week in 2018'\n",
    ")\n",
    "\n",
    "c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the line chart is a great option, we concluded that the best option for this case is the heatmap. This is mainly because it gives an extra layer of detail and this detail complements the visualization for the first question. The color palette chosen also allows people affected by color blindness to be able to draw conclusions without any difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there any areas with a larger number of accidents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the map of New York City diveded in boroughs, we first thought about using a dot map. However, the great amount of instances, the limited space for the visualizations and the restriction of not using interactive plots, made us change our mind after some simple tries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc = alt.Chart(nyc_map).mark_geoshape(\n",
    "    stroke='white',\n",
    "    strokeWidth=1,\n",
    "    filled=True\n",
    ").encode(\n",
    "    color=alt.Color('name',\n",
    "                    scale=alt.Scale(scheme='greys'),\n",
    "                    title='Borough'),\n",
    ").project(\n",
    "    type='identity', reflectY=True\n",
    ")\n",
    "nyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_map_hex = nyc_map.h3.polyfill_resample(8)\n",
    "nyc_map_hex = nyc_map_hex.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nyc_map_hex[['h3_polyfill', 'name', 'geometry']]\n",
    "nyc_map_hex = nyc_map.h3.polyfill_resample(8)\n",
    "nyc_map_hex = nyc_map_hex.reset_index()\n",
    "nyc_map_hex.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to do a choroleth map, but diving the main regions into smaller geometrical forms (hexagons), so the areas would be easier to compare. We use a sequential palette that represnets the quantites aggrupaetd in every hexagon, so the user can see in which areas thers a larger concentration of accidents. We decided to keep the shape of the original map to make orientation easier for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PONER LOS NOMBRES DE LOS BOROUGHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(f'{dir}/nyc_hex_map.json'):\n",
    "#     df = pd.read_json(f'{dir}/nyc_hex_map.json')\n",
    "# else:\n",
    "#     collisions_geo = gpd.GeoDataFrame(collisions, geometry=gpd.points_from_xy(collisions['LONGITUDE'], collisions['LATITUDE']))\n",
    "\n",
    "#     collisions_geo.crs = nyc_map_hex.crs\n",
    "    \n",
    "#     df = collisions_geo.sjoin(nyc_map_hex, how='right')\n",
    "#     # save to csv\n",
    "#     df.to_json(f'{dir}/nyc_hex_map.json', orient='records', na='null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions_geo = gpd.GeoDataFrame(collisions, geometry=gpd.points_from_xy(collisions['LONGITUDE'], collisions['LATITUDE']))\n",
    "\n",
    "collisions_geo.crs = nyc_map_hex.crs\n",
    "\n",
    "df = collisions_geo.sjoin(nyc_map_hex, how='right')\n",
    "\n",
    "tmp = df.groupby(['h3_polyfill', 'name']).count().reset_index()\n",
    "tmp = tmp[['h3_polyfill', 'name', 'COLLISION_ID']]\n",
    "tmp.columns = ['h3_polyfill', 'name', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(tmp, on=['h3_polyfill', 'name'], how='left')\n",
    "\n",
    "df = df[['h3_polyfill', 'name', 'count', 'geometry']]\n",
    "\n",
    "df = df.drop_duplicates(subset=['h3_polyfill', 'name'])\n",
    "\n",
    "c1 = alt.Chart(df).mark_geoshape(\n",
    "    stroke='white',\n",
    "    strokeWidth=1,\n",
    "    filled=True\n",
    ").encode(\n",
    "    color=alt.Color('count:Q',\n",
    "                    # scale=alt.Scale(scheme='greys'),\n",
    "                    # scale=alt.Scale(scheme='tealblues'),\n",
    "                    scale=alt.Scale(range=['#B3E9C7', '#5603ad']),\n",
    "                    title='Borough'),\n",
    ").project(\n",
    "    type='identity', reflectY=True\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "c2 = alt.Chart(nyc_map).mark_geoshape(\n",
    "    stroke='#1d3557',\n",
    "    strokeWidth=1,\n",
    "    filled=False\n",
    ").project(\n",
    "    type='identity', reflectY=True\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "c1 + c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_map_hex.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_map.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select LATITUDE and LONGITUDE columns and convert them to a GeoDataFrame\n",
    "collisions_geo = gpd.GeoDataFrame(collisions[['LATITUDE', 'LONGITUDE', 'BOROUGH', 'ZIP CODE', 'TOTAL INJURED/KILLED']], geometry=gpd.points_from_xy(collisions.LONGITUDE, collisions.LATITUDE))\n",
    "\n",
    "collisions_geo.crs = \"EPSG:4326\"\n",
    "collisions_geo.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_map_hex.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = collisions[['BOROUGH', 'CRASH DATE']].groupby('BOROUGH').count().reset_index().head()\n",
    "df.columns = ['BOROUGH', 'COLLISION COUNT']\n",
    "\n",
    "df2 = collisions[['BOROUGH', 'TOTAL INJURED/KILLED']].groupby('BOROUGH').sum().reset_index().head()\n",
    "\n",
    "df3 = collisions[['BOROUGH', 'LONGITUDE', 'LATITUDE']].groupby('BOROUGH').mean().reset_index().head()\n",
    "df3.columns = ['BOROUGH', 'MEAN LONGITUDE', 'MEAN LATITUDE']\n",
    "\n",
    "df = df.merge(df2, on='BOROUGH', how='left')\n",
    "df = df.merge(df3, on='BOROUGH', how='left')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESTO NO SE QUE WEA ES PERO NO SE ME EJECUTA AS√ç QUE LO COMENT√â"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circles = alt.Chart(df).mark_circle(\n",
    "    tooltip=True,\n",
    "    filled=True,\n",
    "    opacity=1\n",
    ").encode(\n",
    "    x=alt.X('MEAN LONGITUDE:Q', axis=None),\n",
    "    y=alt.Y('MEAN LATITUDE:Q', axis=None),\n",
    "    size=alt.Size('COLLISION COUNT:Q',\n",
    "                  title='Collision Count')\n",
    ").properties(\n",
    "    title='Number of Collisions by Borough'\n",
    ")\n",
    "\n",
    "circles2 = alt.Chart().mark_circle(\n",
    "    tooltip=True,\n",
    "    filled=True,\n",
    "    opacity=1\n",
    ").encode(\n",
    "    x=alt.X('MEAN LONGITUDE:Q', axis=None),\n",
    "    y=alt.Y('MEAN LATITUDE:Q', axis=None),\n",
    "    size=alt.Size('TOTAL INJURED/KILLED:Q',\n",
    "                  title='Total Injured/Killed')\n",
    ").properties(\n",
    "    title='Number of Injured/Killed by Borough'\n",
    ")\n",
    "\n",
    "nyc + circles | nyc + circles2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = pd.DataFrame({\n",
    "    'BOROUGH': ['BRONX', 'BROOKLYN', 'MANHATTAN', 'QUEENS', 'STATEN ISLAND'],\n",
    "    'POPULATION_2018': [1438000000, 2601000000, 1632000000, 2299000000, 474101],\n",
    "    'POPULATION_2020': [1427000000, 2577000000, 1629000000, 2271000000, 475596],\n",
    "    'CAR OWNERSHIP': [0.40, 0.44, 0.22, 0.62, 0.83]\n",
    "})\n",
    "\n",
    "population['MEAN POPULATION'] = population[['POPULATION_2018', 'POPULATION_2020']].mean(axis=1)\n",
    "\n",
    "population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_population = population['MEAN POPULATION'].sum()\n",
    "\n",
    "df = collisions[['BOROUGH', 'CRASH TIME INTERVAL']]\n",
    "df.insert(0, 'COUNT', 1)\n",
    "\n",
    "df = df.groupby(['BOROUGH', 'CRASH TIME INTERVAL']).count().reset_index()\n",
    "df = df.merge(population[['BOROUGH', 'MEAN POPULATION', 'CAR OWNERSHIP']], on='BOROUGH', how='left')\n",
    "\n",
    "df['NORMALIZED COUNT'] = df['COUNT'] * df['CAR OWNERSHIP'] \n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we consider the map to be a pretty good option, we also thought that a linechart representing the normalized count of accidents along the day hours for each borough could also be a good option to answer the proposed question. We normalized the count of accidents with the information of the follwoing [article](https://edc.nyc/article/new-yorkers-and-their-cars), which especifies the percentage of people in every borough owns a car. This also helps give an extra view on the most common hour of the day were accidents occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poner grid en las X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df).mark_line(\n",
    "    tooltip=True,\n",
    "    size=2\n",
    ").encode(\n",
    "    x=alt.X('CRASH TIME INTERVAL:N',\n",
    "            title='Crash Time Interval',\n",
    "            axis=alt.Axis(labelAngle=0)),\n",
    "    y=alt.Y('NORMALIZED COUNT:Q',\n",
    "            title='Number of Collisions normalized by Car Ownership Data'),\n",
    "    color=alt.Color('BOROUGH:N',\n",
    "                    scale=alt.Scale(range=['#a3ffd6', '#d69bf5', '#ff8080', '#80ff80', '#80bfff']),\n",
    "                    title='Borough')\n",
    ").properties(\n",
    "    title='Normalized Number of Collisions by Time Interval'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DECISION FINAL: USAR LOS DOS O SOLO EL MAPA, YA VEMOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there a correlation between weather conditions and accidents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_aggregated['DATE'] = pd.to_datetime(weather_aggregated['DATE']).dt.strftime('%Y-%m-%d')\n",
    "weather_aggregated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions['CRASH DATE'] = pd.to_datetime(collisions['CRASH DATE']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "collisions_by_day = collisions[['CRASH DATE', 'COLLISION_ID']].groupby(['CRASH DATE']).count().reset_index()\n",
    "collisions_by_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(weather_aggregated, collisions_by_day, left_on='DATE', right_on='CRASH DATE')\n",
    "merged_data.drop(columns=['CRASH DATE'], inplace=True)\n",
    "merged_data.rename(columns={'COLLISION_ID': 'COLLISION COUNT'}, inplace=True)\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['BEAUFORT_SCALE'] = merged_data['BEAUFORT_SCALE'].astype('category')\n",
    "merged_data['PRCP_SCALE'] = merged_data['PRCP_SCALE'].astype('category')\n",
    "\n",
    "# min temperature is 11, max temperature is 29\n",
    "bin_edges = list(range(10, 30, 2))\n",
    "merged_data['TEMP_SCALE'] = pd.cut(merged_data['MEAN_TEMP'], bins=bin_edges, right=False)\n",
    "merged_data['CASES_COUNT'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_data = merged_data.groupby(['BEAUFORT_SCALE', 'TEMP_SCALE']).agg({'COLLISION COUNT': 'sum', 'CASES_COUNT': 'sum'}).reset_index()\n",
    "\n",
    "# change teh column type to categorical\n",
    "agg_data['BEAUFORT_SCALE'] = agg_data['BEAUFORT_SCALE'].astype('category')\n",
    "agg_data['TEMP_SCALE'] = agg_data['TEMP_SCALE'].astype('str')\n",
    "\n",
    "agg_data['NORMALIZED_COLLISION_COUNT'] = agg_data['COLLISION COUNT'] / agg_data['CASES_COUNT']\n",
    "\n",
    "agg_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nan of NORMALIZED_COLLISION_COUNT with 0\n",
    "#agg_data['NORMALIZED_COLLISION_COUNT'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that in summertime there is nearly no rain, we decided for this first visualization to only analyze the data given the categorized average daily speed of wind and the average daily temperature. Since we want to encode two keys, wind and temperature, and a value, number of crashes, a heatmap seems to be a good option. To do so we also categorized the temperature taking into account the maximum and minimum temperatures registered. The result is correct and relatively easy to interpret, but at the same time there are values missing for some combinations, which is expectable since the climate does not usually vary much in the same season. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(agg_data).mark_rect().encode(\n",
    "    x=alt.X('TEMP_SCALE:O',\n",
    "            title='Temperature Scale',\n",
    "            axis=alt.Axis(labelAngle=0)),\n",
    "    y=alt.Y('BEAUFORT_SCALE:N',\n",
    "            \n",
    "            title='Beaufort Scale'),\n",
    "    color=alt.Color('NORMALIZED_COLLISION_COUNT:Q',\n",
    "                    scale=alt.Scale(range=['#f0fff1', '#5603ad']),\n",
    "                    title='Collision Count')\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=300,\n",
    "    title='Collision Count by Beaufort Scale and Precipitation Scale'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the file if it doesn't exist\n",
    "if not os.path.exists(f'{dir}/merged_data.csv'):\n",
    "    merged_data.to_csv(f'{dir}/merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other option we consider to be useful is a small multiples of the scatterplots of collisions versus the three weather variables (precipitation, temperature and wind speed). This visualization is simple and clear to show if there is any correlation between the number of accidents and this meterological factors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = alt.Chart(merged_data).mark_point(\n",
    "    filled=True,\n",
    "    size=100,\n",
    "    opacity=0.5\n",
    ").encode(\n",
    "    x=alt.X('MEAN_TEMP:Q',\n",
    "            title='Mean Temperature',\n",
    "            scale=alt.Scale(domain=[10, 30])),\n",
    "    y=alt.Y('COLLISION COUNT:Q',\n",
    "            title='Number of Collisions'),\n",
    "    color=alt.Color('year(DATE):N',\n",
    "                    title='Year')\n",
    ").properties(\n",
    "    title='Number of Collisions by Mean Temperature'\n",
    ")\n",
    "\n",
    "t2 = alt.Chart(merged_data).mark_point(\n",
    "    filled=True,\n",
    "    size=100,\n",
    "    opacity=0.5\n",
    ").encode(\n",
    "    x=alt.X('PRCP:Q',\n",
    "            title='Precipitation'),\n",
    "    y=alt.Y('COLLISION COUNT:Q',\n",
    "            title='Number of Collisions'),\n",
    "    color=alt.Color('year(DATE):N',\n",
    "                    title='Year'),\n",
    ").properties(\n",
    "    title='Number of Collisions by Precipitation'\n",
    ")\n",
    "\n",
    "t3 = alt.Chart(merged_data).mark_point(\n",
    "    filled=True,\n",
    "    size=100,\n",
    "    opacity=0.5\n",
    ").encode(\n",
    "    x=alt.X('AWND:Q',\n",
    "            scale=alt.Scale(domain=[1, 6.5]),\n",
    "            title='Wind Speed'),\n",
    "    y=alt.Y('COLLISION COUNT:Q',\n",
    "            title='Number of Collisions'),\n",
    "    color=alt.Color('year(DATE):N',\n",
    "                    scale=alt.Scale(range=['#A7C9C7', '#8367C7']),\n",
    "                    title='Year'),\n",
    ").properties(\n",
    "    title='Number of Collisions by Wind Speed'\n",
    ")\n",
    "\n",
    "t1 | t2 | t3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are alredy using a heatmap for another question, we decided to use the small multiples for the scatterplots. As mentioned, this option is effective and really allows the user to see if there is any clear correlation. The main disadvantage of this plot is that it may occupy more space than other options."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
