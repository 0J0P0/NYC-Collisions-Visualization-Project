{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Authors**:\n",
    "-  *Juan P. Zaldivar E.*\n",
    "-  *Enrique Mill√°n X.*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the preprocessing required for the datasets. The datasets are:\n",
    "- Colissions dataset.\n",
    "- Weather dataset.\n",
    "- New York Map.\n",
    "\n",
    "\n",
    "<!-- explicar que primer se hace una exploracion visual y luego el preprocesing? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset obtention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [*collision*](https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95) dataset was already given by the instructors of the project.\n",
    "\n",
    "The *weather* dataset was obtained following the next steps:\n",
    "\n",
    "- Visit the [NOAA Climate Data Online Search](https://www.ncdc.noaa.gov/cdo-web/search) web page.\n",
    "\n",
    "- Select the following options:\n",
    "  - `Weather Observation Type/Dataset -> Daily Summaries, Date Range -> 2018-01-01 to 2020-12-31, Search For -> Cities, Search Term -> New York City.`\n",
    "\n",
    "- Look for \"*New York, NY US*\" and click in ADD TO CART. Now, click the cart in the top right corner.\n",
    "\n",
    "- Select \"*Custom GHCN-Daily CSV*\", and the date previously selected (2018-01-01 to 2020-12-31). We are selecting more information than needed (to avoid disjoint downloads), but we will later filter it with ``Pandas`` and ``Open Refine``. Click continue.\n",
    "\n",
    "- Fill the three options, and select \"*metric units*\".\n",
    "\n",
    "- Fill all the options remaining and click continue. There are some options that will be probably not needed, but we will further analyze this when cleaning the datasets.\n",
    "\n",
    "- Type the email where you want to receive the data so the order can start.\n",
    "\n",
    "The *map* dataset was obtained following the next steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are located in the folder `Data/` and the results are saved in the folder `Data/Preprocessed/`. Following are the loading of each dataset and the import of the required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from Modules import preprocessing as pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# comentar las versiones de los paquetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing of the files was done conjointly with OpenRefine and the proposed python libraries in order to be able to take advantages of both tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dir = './Data'\n",
    "temp_pre = './Data/tmp_pre'\n",
    "colission_exists = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collision dataset preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration?\n",
    "\n",
    "The collision table incorporates information pertaining to individual crash events, with each row representing a distinct collision incident. The dataframe compile details from all police reported motor vehicle collisions in NYC. \n",
    "\n",
    "<!-- ...se hace una breve descripcion de la exploracion? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset was first loaded into a ``Pandas`` dataframe in order to filter the desired range of dates. The reason lays in a more efficient way to filter the data, taking the size of the original dataset into account. This volumn of data made the computational process in OpenRefine very slow and ineffcient. After this initial filtering, the dataset was exported to a ``.csv`` file and loaded into OpenRefine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'{dir}/collisions_2018-2020.csv'):\n",
    "    collision = pd.read_csv(f'{dir}/collisions_2018-2020.csv')\n",
    "    colission_exists = True\n",
    "else:\n",
    "    collision = pd.read_csv(f'{dir}/collisions.csv')\n",
    "\n",
    "collision.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the filtered version does not exist, we proceed with the filtering. To filter the data to the summer of 2018 and 2020, the first step is to change the data type of the **CRASH DATE** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not colission_exists:\n",
    "    collision['CRASH DATE'] = pd.to_datetime(collision['CRASH DATE'])\n",
    "\n",
    "    collision = collision[((collision['CRASH DATE'] >= '2018-06-01') & (collision['CRASH DATE'] <= '2018-09-30')) | ((collision['CRASH DATE'] >= '2020-06-01') & (collision['CRASH DATE'] <= '2020-09-30'))]\n",
    "    collision.to_csv(f'{dir}/collisions_2018-2020.csv', index=False)\n",
    "\n",
    "collision.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the filtering, the dataset was exported to a ``.csv`` file and loaded into OpenRefine. The procedure and reasoning taken in OpenRefine will be explained and justified in the present section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data type conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data conversion was done in OpenRefine. The **CRASH DATE** attribute was converted to a date type. The **COLLISION ID** and **CRASH TIME** were set as strings for the time being.\n",
    "\n",
    " The attributes relating the greographic location of the collision were set as strings with some special remarks. All the values were set to uppercase and the extra spaces were removed (if any). The reason for this was to ease the work of the clusterization method used to collectively inspect and edit cells in the case that there were some values that were not correctly or consistently written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the attributes refering to the number of persons involved in the collision, the data type was set to integer. The reason for this is that the values are discrete and the values are not expected to be negative.\n",
    "\n",
    "The vehicle and factors attributes were left as strings for the time being. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data selection and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the following transformations were applied with OpenRefine, but some comprobations to justify the transformations are carried on in this section of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision = pd.read_csv(f'{dir}/collisions_2018-2020_prepro_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision[\"CRASH DATE\"] = pd.to_datetime(precollision[\"CRASH DATE\"])\n",
    "precollision[\"DAY NAME\"] = precollision[\"CRASH DATE\"].dt.day_name()\n",
    "# precollision[\"TYPE OF DAY\"] = np.where(precollision[\"DAY NAME\"].isin([\"Saturday\", \"Sunday\"]), \"Weekend\", \"Weekday\")\n",
    "\n",
    "# move DAY NAME to the second column\n",
    "cols = precollision.columns.tolist()\n",
    "cols = cols[:1] + cols[-1:] + cols[1:-1]\n",
    "precollision = precollision[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision['CRASH TIME'] = pd.to_datetime(precollision['CRASH TIME'], format='%H:%M').dt.time\n",
    "precollision['CRASH TIME INTERVAL'] = precollision['CRASH TIME'].apply(lambda x: f\"{x.hour:02}\")\n",
    "precollision.drop(columns=['CRASH TIME'], inplace=True)\n",
    "\n",
    "def categorize_moment(hour):\n",
    "    if 5 <= int(hour) < 12:\n",
    "        return \"Morning\"\n",
    "    elif 12 <= int(hour) < 18:\n",
    "        return \"Afternoon\"\n",
    "    else:\n",
    "        return \"Night\"\n",
    "    \n",
    "precollision['CRASH MOMENT'] = precollision['CRASH TIME INTERVAL'].apply(categorize_moment)\n",
    "\n",
    "# move TIME INTERVAL to the fourth column\n",
    "cols = precollision.columns.tolist()\n",
    "cols = cols[:3] + cols[-1:] + cols[3:-1]\n",
    "precollision = precollision[cols]\n",
    "\n",
    "# move CRASH MOMENT to the fifth column\n",
    "cols = precollision.columns.tolist()\n",
    "cols = cols[:4] + cols[-1:] + cols[4:-1]\n",
    "precollision = precollision[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geographical attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, **ON STREET NAME** and **OFF STREET NAME** seem to be the same attribute, but with different names. The web site of the dataset cointains the following descriptions:\n",
    "\n",
    "- **ON STREET NAME**: *Street on which the collision occurred*.\n",
    "- **OFF STREET NAME**: *Street address if known*.\n",
    "\n",
    "This gives the idea that both attributes probably contain the same information. Furthermore, there are no rows with both attributes filled, which makes the idea of merging both attributes plausible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision[(collision['ON STREET NAME'].notnull()) & (collision['OFF STREET NAME'].notnull())].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision[(collision['ON STREET NAME'].notnull()) | (collision['OFF STREET NAME'].notnull())].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting attribute after merging both columns is called **STREET NAME** and contains the street name/address where the collision occurred, with no missing values. Some rows will have a more detailed description of the street, while others will only have the name of the street. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CROSS STREET**, which is the third attribute related to the street enviroment, and represents the name of the closest street crossing the street of the crash, can be dropped since it is not useful for the intended analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, **LOCATION** seems to contain the tuple (**LATITUDE**, **LONGITUDE**), so we could, a priori, remove the two extra attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision[(collision['LOCATION'].notnull()) & (collision['LATITUDE'].notnull()) & (collision['LONGITUDE'].notnull())].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows where the three attributes are not missing does not cover the total number of rows, but there are no rows where the **LOCATION** attribute is missing and at least one of the other two attributes is not missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision[(collision['LOCATION'].isnull()) & (collision['LATITUDE'].isnull()) & (collision['LONGITUDE'].notnull()) | (collision['LATITUDE'].notnull()) & (collision['LONGITUDE'].isnull())].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision[(collision['LOCATION'].isnull())].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which makes the rest of the rows (7667) with missing values in the three attributes. This means that the **LATITUDE** and **LONGITUDE** attributes can be removed, since the **LOCATION** attribute contains the same information. With this transformation, the number of attributes is reduced by two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusterization was done with the *key collision* method and the *fingerprint* keying function. No significant variations were found after a copuble iterations in the values of the attributes but misspellings were found. The misspellings were corrected and the clusterization was done again. The results were the same, which means that the values were already consistent. To verify the result, a Neares Neighbours analysis was done as well but without finding any significant variation.\n",
    "\n",
    "--> mas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vehicle attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding vehicle information, the statement of the project specifies that only the **VEHICLE CODE TYPE 1** is of interest to the visualization, so all the other vehicle codes can be removed (2-5). With this, the cotributing factors of the other vehicles can also be removed.\n",
    "\n",
    "We have seen already that there are many classes of the **VEHICLE CODE TYPE 1** values, so we reduced the number of classes by clusterization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(collision['VEHICLE TYPE CODE 1'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the clusterization (key collision and fingerprint keying function) of the **VEHICLE TYPE CODE 1** attribute we found a lot of misspellings and inconsistencies. The clusterization was done iteratively, correcting the misspellings and inconsistencies found in each iteration. After a couple of iterations, where a lot of subclasses were merged into the same type of vehicle thus aggregating more the data, the number of unique clasess was reduced to the point where the last subclasses manually could be merged manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision['VEHICLE TYPE CODE 1'] = precollision['VEHICLE TYPE CODE 1'].replace(\n",
    "['SUV', 'FLAT', '3-DOOR', 'CHEVY EXPR', 'PC', 'ELEC. UNIC', 'E REVEL SC','F150XL PIC', '2- TO', 'NEW Y', 'STREE', 'RGS', 'OMR', 'DEMA-', 'BK', 'NYPD'], 'CAR')\n",
    "\n",
    "precollision['VEHICLE TYPE CODE 1'] = precollision['VEHICLE TYPE CODE 1'].replace(\n",
    "    ['99999'], 'UNKNOWN')\n",
    "\n",
    "precollision['VEHICLE TYPE CODE 1'].fillna('UNKNOWN', inplace=True)\n",
    "\n",
    "precollision['VEHICLE TYPE CODE 1'] = precollision['VEHICLE TYPE CODE 1'].replace(\n",
    "    ['BULK AGRICULTURE', 'PK', 'TANK', 'SLINGSHOT', 'UTV', 'JOHN DEERE', '1C', 'STAK', 'PALLET', 'SPRIN', 'ACCES'], 'OTHERS')\n",
    "\n",
    "precollision['VEHICLE TYPE CODE 1'] = precollision['VEHICLE TYPE CODE 1'].replace(\n",
    "    ['BOX', 'DOT EQUIPM', 'DRILL RIG', 'PAS', 'LOADE', 'SGWS', 'HEAVY'], 'TRUCK')\n",
    "\n",
    "precollision['VEHICLE TYPE CODE 1'] = precollision['VEHICLE TYPE CODE 1'].replace(\n",
    "    ['MOTORIZED HOME', 'CHASSIS CAB', 'SWT', 'MESSAGE SI', 'RV', 'UHAUL', 'POSTO'], 'VAN')\n",
    "\n",
    "precollision['VEHICLE TYPE CODE 1'] = precollision['VEHICLE TYPE CODE 1'].replace(\n",
    "    ['MOPED', 'J1'], 'MOTORCYCLE')\n",
    "\n",
    "precollision['VEHICLE TYPE CODE 1'] = precollision['VEHICLE TYPE CODE 1'].replace(\n",
    "    ['SANIT'], 'AMBULANCE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting classes of the **VEHICLE CODE TYPE 1** attribute are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(precollision['VEHICLE TYPE CODE 1'].unique()), precollision['VEHICLE TYPE CODE 1'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This types of vehicles were selected so there wasn't too much nor too many classes and the user could still get an insight of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision['VEHICLE TYPE CODE 1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar strategy was done with the **CONTRIBUTING FACTOR VEHICLE 1** attribute. However, the aggrgation was not so exhaustive since this attribute wasn't needed a priori for the main questions that the visualizations should answer. For this attribute basic merge transformations were applied in OpenRefine until no \"strange\" or \"uninformative\" nor repeated classes remained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(precollision['CONTRIBUTING FACTOR VEHICLE 1'].unique()), precollision['CONTRIBUTING FACTOR VEHICLE 1'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of persons attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the visualization purposes, the differentantion of **PERSONS**, **PEDESTRIANS**, **CYCLISTS** and **MOTORISTS** (**INJURED/KILLED**) is irrelevant. A more useful attribute would be the total number of persons involved in the collision. This can be obtained by summing the four attributes under the assumption that the **PERSONS** attribute is not the sum of the other three attributes. \n",
    "\n",
    "This condition was needed to be checked because the documentation of the dataset was not precise enough to determinate if **NUMBER OF PERSON INJURED/KILLED** was an aggregate from the other three columns or not.\n",
    "\n",
    "*Note: The metadata information available in the web of the dataset was: \"Number of persons injured/killed\" regarding the **NUMBER OF PERSONS INJURED/KILLED**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision['NUMBER OF PERSONS INJURED'].equals(collision['NUMBER OF PEDESTRIANS INJURED'] + collision['NUMBER OF CYCLIST INJURED'] + collision['NUMBER OF MOTORIST INJURED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision['NUMBER OF PERSONS INJURED'].equals(collision['NUMBER OF PEDESTRIANS INJURED'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen by the logical comprobations, the **NUMBER OF PERSONS INJURED/KILLED** is not the sum of the other three attributes. Furthermore, the terms persons and pedestrians are not equal, as one could have thought that the term persons was used to refer to pedestrians.\n",
    "\n",
    "Based on this, the discrete attributes refering to the injured people were summed to obtain **NUMBER OF INJURED** and the discrete attributes refering to the killed people were summed to obtain **NUMBER OF KILLED**. The **NUMBER OF INJURED/KILLED** attributes were removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenRefine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the dataset contains the attributes needed (with the *weather* attributes as an exception) for the analysis and some extra attributes that were considered interesting for some possible extra analysis or insights that we could think about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has already been mentioned the existence of some missing values. In the previous section, the verification of missing values was done with the ``.isnull()`` method of ``Pandas``. However, this method does not take into account the ``NaN`` values. In order to check the existence of ``NaN`` values, the ``.isna()`` method was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = (precollision.isnull().sum() == precollision.isna().sum())\n",
    "comp[comp == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen previously, all the missing values of the dataset are detected both with ``.isnull()`` and ``.isna()``. After this check, we could group the attributes with missign values in three separeted clusters:\n",
    "- Geographical attributes\n",
    "- Injured/Killed attributes\n",
    "- Vehicle attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation of geographic attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first cluster is formed with reference to the geographicals attributes. The attributes in this cluster are:\n",
    "- **BOROUGH**\n",
    "- **ZIP CODE**\n",
    "- **LOCATION**\n",
    "- **STREET NAME**\n",
    "- **CROSS STREET NAME**\n",
    "\n",
    "Notice that the attributes with the less missing values is **STREET NAME** with only a $0.20\\%$ of the entire dataset, partially thanks to the merge of **ON STREET** and **OFF STREET** attributes in the previous sections. We used this information to impute the missing coordinates of the **LOCATION** attribute and the rest of the attributes sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision['LOCATION'].isnull().sum(), precollision['STREET NAME'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a longitude and latitude column from the coordinates column\n",
    "precollision[['LATITUDE', 'LONGITUDE']] = precollision['LOCATION'].str.split(', ', expand=True)\n",
    "precollision['LATITUDE'] = precollision['LATITUDE'].str.replace('(', '')\n",
    "precollision['LONGITUDE'] = precollision['LONGITUDE'].str.replace(')', '')\n",
    "\n",
    "# convert the columns to float\n",
    "precollision['LATITUDE'] = precollision['LATITUDE'].astype(float)\n",
    "precollision['LONGITUDE'] = precollision['LONGITUDE'].astype(float)\n",
    "\n",
    "precollision.drop(columns=['LOCATION'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_map = gpd.read_file('Data/new-york-city-boroughs-ny_.geojson')\n",
    "\n",
    "boroughs = ['Bronx', 'Brooklyn', 'Manhattan', 'Queens', 'Staten Island']\n",
    "\n",
    "borough_polly = {}\n",
    "for b in boroughs:\n",
    "    poly = nyc_map[nyc_map['name'] == b]['geometry']\n",
    "    borough_polly[b] = poly.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in precollision.iterrows():\n",
    "    if row['BOROUGH'] is not None:\n",
    "        lon = row['LONGITUDE']\n",
    "        lat = row['LATITUDE']\n",
    "\n",
    "        if lat is not None and lon is not None:\n",
    "            p = Point(lon, lat)\n",
    "            for b, poly in borough_polly.items():\n",
    "                if p.within(poly):\n",
    "                    precollision.loc[idx, 'BOROUGH'] = b.upper()\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision['ZIP CODE'].isnull().sum(), precollision['BOROUGH'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPLICAR EL PORQ HAY MENOS MISSING VALUES EN BOROUGH QUE EN LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision[(precollision['LATITUDE'].isna()) & (precollision['BOROUGH'].isna())].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation of vehicle attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poner unespecified en factor $i$ si el vehiculo $i$ es no null y factor $i$ es null\n",
    "# o\n",
    "# poner unknown en vehiculo $i$ si el vehiculo $i$ es null y factor $i$ no es null\n",
    "\n",
    "# son disjuntas estas operaciones?????????????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some rows of the dataset, the **CONTRIBUTING FACTOR VEHICLE** is missing but the **VEHICLE TYPE CODE** is not. This suggests that the vehicle type is known, but the factor that contributed to the collision is not. In order to fill this missing values, the factor was set as *unespecified*. This was done for all the rows and columns where the **CONTRIBUTING FACTOR VEHICLE** was missing with the above condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.imputation_with_ref_col(precollision, 'CONTRIBUTING FACTOR VEHICLE 1', 'VEHICLE TYPE CODE 1', 'Unspecified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, in some rows of the dataset, the **VEHICLE TYPE CODE** is missing but the **CONTRIBUTING FACTOR VEHICLE** is not. This suggests that the factor that contributed to the collision is known, but the vehicle type is not. In order to fill this missing values, the vehicle type was set as *unknown*. This was done for all the rows and columns where the **VEHICLE TYPE CODE** was missing with the above condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.imputation_with_ref_col(precollision, 'VEHICLE TYPE CODE 1', 'CONTRIBUTING FACTOR VEHICLE 1', 'UNKNOWN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- comentar la reduccion de missing values -->\n",
    "\n",
    "Notice that the only missing values in the **CONTRIBUTING FACTOR VEHICLE** attribute and **VEHICLE TYPE CODE** attribute are in the same rows. This means that the number of missing values in the **CONTRIBUTING FACTOR VEHICLE** attribute and **VEHICLE TYPE CODE** attribute is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# como se hace para imputar los missing de los dos campos a la vez?\n",
    "\n",
    "# code 1 como unespecified/unknown\n",
    "# code resto no existent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation of number of person attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# como se hace para imputar los missing del numero de personas? o mejor se elimina?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision[precollision['NUMBER OF INJURED'].isnull() | precollision['NUMBER OF KILLED'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the resulting rows with missing values of the number of persons involved in the collision are a small fraction of the total dataset, we decided to set them to 0, assuming that there were no persons involved in the collision.\n",
    "\n",
    "Given the small percentage of the rows $0.01\\%$, it was considered that there would not be a significant impact in the final visualization whether the missing values were set to 0 or the rows with missing values were dropped. However, we decided to set the missing values to 0 in order to keep the rows and not lose some of its information. This was done also because **NUMBER OF INJURED** and  **NUMBER OF KILLED** were attributes that are not necessary for the main visualizations and we were only keeping them in case we found an intereseting extra visualization with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision['NUMBER OF INJURED'].fillna(0, inplace=True)\n",
    "precollision['NUMBER OF KILLED'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this done, the only meaningful missing values are the 226 of **STREET NAME**, that is because the other location related attributes could be mostly imputed with **STREET NAME**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'{dir}/collisions_clean.csv'):\n",
    "    precollision.to_csv(f'{dir}/collisions_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precollision.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
